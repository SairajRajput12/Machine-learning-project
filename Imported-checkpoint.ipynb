{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b187a679-56d4-4bd2-8f9c-a3a6b1c0b6ee",
   "metadata": {},
   "source": [
    "## Code for action recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9d4a433c-bc0b-4e75-8c22-ddeb2005c214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    nose_X    nose_Y    nose_Z    nose_V  left_eye_inner_X  left_eye_inner_Y  \\\n",
      "0 -0.26861 -0.458431  0.328992  0.999988         -0.260716         -0.488644   \n",
      "\n",
      "   left_eye_inner_Z  left_eye_inner_V  left_eye_X  left_eye_Y  ...  \\\n",
      "0          0.253323          0.999979   -0.256325    -0.49046  ...   \n",
      "\n",
      "   right_heel_Z  right_heel_V  left_foot_index_X  left_foot_index_Y  \\\n",
      "0      0.336425      0.988631           0.512082           0.486277   \n",
      "\n",
      "   left_foot_index_Z  left_foot_index_V  right_foot_index_X  \\\n",
      "0          -0.475314           0.997432           -0.352852   \n",
      "\n",
      "   right_foot_index_Y  right_foot_index_Z  right_foot_index_V  \n",
      "0            0.575511            0.301457             0.98793  \n",
      "\n",
      "[1 rows x 132 columns]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "`tf.data.Dataset` only supports Python-style iteration in eager mode or within tf.function.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 73\u001b[0m\n\u001b[0;32m     69\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([np\u001b[38;5;241m.\u001b[39marray(pre_lm)\u001b[38;5;241m.\u001b[39mflatten()], columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \n\u001b[0;32m     70\u001b[0m                                                             \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m landmark_names \n\u001b[0;32m     71\u001b[0m                                                             \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mZ\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mV\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(data)\n\u001b[1;32m---> 73\u001b[0m predict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mmax\u001b[39m(predict) \u001b[38;5;241m>\u001b[39m threshold:\n\u001b[0;32m     76\u001b[0m     pose_class \u001b[38;5;241m=\u001b[39m class_names[np\u001b[38;5;241m.\u001b[39margmax(predict)]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:503\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m iterator_ops\u001b[38;5;241m.\u001b[39mOwnedIterator(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 503\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration in eager mode or within tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: `tf.data.Dataset` only supports Python-style iteration in eager mode or within tf.function."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from keras.models import load_model\n",
    "\n",
    "source = \"Running1.jpg\"\n",
    "path_saved_model = \"Models/Action Recognition/model.keras\"\n",
    "threshold = 0.2\n",
    "save = \"Output\"\n",
    "\n",
    "torso_size_multiplier = 2.5\n",
    "n_landmarks = 33\n",
    "landmark_names = [\n",
    "    'nose',\n",
    "    'left_eye_inner', 'left_eye', 'left_eye_outer',\n",
    "    'right_eye_inner', 'right_eye', 'right_eye_outer',\n",
    "    'left_ear', 'right_ear',\n",
    "    'mouth_left', 'mouth_right',\n",
    "    'left_shoulder', 'right_shoulder',\n",
    "    'left_elbow', 'right_elbow',\n",
    "    'left_wrist', 'right_wrist',\n",
    "    'left_pinky_1', 'right_pinky_1',\n",
    "    'left_index_1', 'right_index_1',\n",
    "    'left_thumb_2', 'right_thumb_2',\n",
    "    'left_hip', 'right_hip',\n",
    "    'left_knee', 'right_knee',\n",
    "    'left_ankle', 'right_ankle',\n",
    "    'left_heel', 'right_heel',\n",
    "    'left_foot_index', 'right_foot_index',\n",
    "]\n",
    "class_names = ['Running', 'Standing', 'Walking', 'Waving']\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "# Load saved model\n",
    "model = load_model(path_saved_model, compile=True)\n",
    "\n",
    "# Load sample Image\n",
    "img = cv2.imread(source)\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "result = pose.process(img_rgb)\n",
    "\n",
    "if result.pose_landmarks:\n",
    "    lm_list = []\n",
    "    for landmarks in result.pose_landmarks.landmark:\n",
    "        lm_list.append(landmarks)\n",
    "\n",
    "    center_x = (lm_list[landmark_names.index('right_hip')].x +\n",
    "                lm_list[landmark_names.index('left_hip')].x) * 0.5\n",
    "    center_y = (lm_list[landmark_names.index('right_hip')].y +\n",
    "                lm_list[landmark_names.index('left_hip')].y) * 0.5\n",
    "\n",
    "    max_distance = max(math.sqrt((lm.x - center_x) ** 2 + (lm.y - center_y) ** 2) \n",
    "                       for lm in lm_list)\n",
    "    torso_size = math.sqrt((lm_list[landmark_names.index('right_shoulder')].x - center_x) ** 2 +\n",
    "                           (lm_list[landmark_names.index('right_shoulder')].y - center_y) ** 2)\n",
    "    max_distance = max(torso_size * torso_size_multiplier, max_distance)\n",
    "\n",
    "    pre_lm = [[(lm.x - center_x) / max_distance, \n",
    "               (lm.y - center_y) / max_distance,\n",
    "               lm.z / max_distance,\n",
    "               lm.visibility] for lm in lm_list]\n",
    "\n",
    "    data = pd.DataFrame([np.array(pre_lm).flatten()], columns=[f'{name}_{attr}' \n",
    "                                                                for name in landmark_names \n",
    "                                                                for attr in ['X', 'Y', 'Z', 'V']])\n",
    "    print(data)\n",
    "    predict = model.predict(data)[0]\n",
    "\n",
    "    if max(predict) > threshold:\n",
    "        pose_class = class_names[np.argmax(predict)]\n",
    "        print('Predictions:', predict)\n",
    "        print('Predicted Pose Class:', pose_class)\n",
    "    else:\n",
    "        pose_class = 'Unknown Pose'\n",
    "        print('Predictions is below given Confidence!!')\n",
    "else:\n",
    "    pose_class = 'No Pose Detected'\n",
    "\n",
    "print('Predicted Pose Class:', pose_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "83f040ae-49ec-460a-838f-24abb149388b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ImageDataGenerator' from 'keras.preprocessing.image' (C:\\Users\\saira\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\preprocessing\\image\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m TrainingImagePath\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/data/Train\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageDataGenerator\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Understand more about ImageDataGenerator at below link\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# of the original image, which leads to a better model, since it learns\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# on the good and bad mix of images\u001b[39;00m\n\u001b[0;32m     11\u001b[0m train_datagen \u001b[38;5;241m=\u001b[39m ImageDataGenerator(\n\u001b[0;32m     12\u001b[0m         shear_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[0;32m     13\u001b[0m         zoom_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[0;32m     14\u001b[0m         horizontal_flip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'ImageDataGenerator' from 'keras.preprocessing.image' (C:\\Users\\saira\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\preprocessing\\image\\__init__.py)"
     ]
    }
   ],
   "source": [
    "TrainingImagePath='/data/Train'\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "# Understand more about ImageDataGenerator at below link\n",
    "# https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
    "\n",
    "# Defining pre-processing transformations on raw images of training data\n",
    "# These hyper parameters helps to generate slightly twisted versions\n",
    "# of the original image, which leads to a better model, since it learns\n",
    "# on the good and bad mix of images\n",
    "train_datagen = ImageDataGenerator(\n",
    "        shear_range=0.1,\n",
    "        zoom_range=0.1,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# Defining pre-processing transformations on raw images of testing data\n",
    "# No transformations are done on the testing images\n",
    "test_datagen = ImageDataGenerator()\n",
    "\n",
    "# Generating the Training Data\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "        TrainingImagePath,\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    "\n",
    "\n",
    "# Generating the Testing Data\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "        TrainingImagePath,\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    "\n",
    "# Printing class labels for each face\n",
    "test_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "619b85ef-0b2e-4139-b208-dcd9df68d37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 9\n",
      "Line 25\n",
      "Line 29\n",
      "line 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saira\\AppData\\Local\\Temp\\ipykernel_20272\\1256554830.py:28: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  (images, labels) = [numpy.array(lst) for lst in [images, labels]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am Home lander\n",
      "75.39244149861042\n",
      "2\n",
      "not recognized\n",
      "Confidence: 75.39244149861042\n",
      "71.1577913804092\n",
      "2\n",
      "not recognized\n",
      "Confidence: 71.1577913804092\n",
      "67.52163321907368\n",
      "2\n",
      "detected\n",
      "Recognized: Tushar, Confidence: 67.52163321907368\n",
      "76.008646314534\n",
      "2\n",
      "not recognized\n",
      "Confidence: 76.008646314534\n",
      "69.56168855257451\n",
      "2\n",
      "detected\n",
      "Recognized: Tushar, Confidence: 69.56168855257451\n",
      "71.71826254579787\n",
      "2\n",
      "not recognized\n",
      "Confidence: 71.71826254579787\n",
      "68.98139899806165\n",
      "2\n",
      "detected\n",
      "Recognized: Tushar, Confidence: 68.98139899806165\n",
      "70.74695557727314\n",
      "2\n",
      "not recognized\n",
      "Confidence: 70.74695557727314\n",
      "70.35732184642796\n",
      "2\n",
      "not recognized\n",
      "Confidence: 70.35732184642796\n",
      "69.32583756345863\n",
      "2\n",
      "detected\n",
      "Recognized: Tushar, Confidence: 69.32583756345863\n",
      "66.49597644942855\n",
      "2\n",
      "detected\n",
      "Recognized: Tushar, Confidence: 66.49597644942855\n",
      "63.2973307216785\n",
      "2\n",
      "detected\n",
      "Recognized: Tushar, Confidence: 63.2973307216785\n",
      "60.39451350389914\n",
      "2\n",
      "detected\n",
      "Recognized: Tushar, Confidence: 60.39451350389914\n",
      "91.82304491607843\n",
      "2\n",
      "not recognized\n",
      "Confidence: 91.82304491607843\n",
      "66.13685685055563\n",
      "2\n",
      "detected\n",
      "Recognized: Tushar, Confidence: 66.13685685055563\n",
      "73.9273870553545\n",
      "2\n",
      "not recognized\n",
      "Confidence: 73.9273870553545\n",
      "68.03138723097524\n",
      "2\n",
      "detected\n",
      "Recognized: Tushar, Confidence: 68.03138723097524\n",
      "92.23028079595538\n",
      "2\n",
      "not recognized\n",
      "Confidence: 92.23028079595538\n",
      "59.64989349186601\n",
      "2\n",
      "detected\n",
      "Recognized: Tushar, Confidence: 59.64989349186601\n",
      "68.35621604809266\n",
      "2\n",
      "detected\n",
      "Recognized: Tushar, Confidence: 68.35621604809266\n",
      "75.71967116473971\n",
      "2\n",
      "not recognized\n",
      "Confidence: 75.71967116473971\n",
      "71.72632021731512\n",
      "2\n",
      "not recognized\n",
      "Confidence: 71.72632021731512\n",
      "74.66738634876366\n",
      "2\n",
      "not recognized\n",
      "Confidence: 74.66738634876366\n",
      "72.59463306239162\n",
      "2\n",
      "not recognized\n",
      "Confidence: 72.59463306239162\n",
      "69.59722316977393\n",
      "2\n",
      "detected\n",
      "Recognized: Tushar, Confidence: 69.59722316977393\n",
      "73.10150995537902\n",
      "2\n",
      "not recognized\n",
      "Confidence: 73.10150995537902\n",
      "68.47885073691715\n",
      "2\n",
      "detected\n",
      "Recognized: Tushar, Confidence: 68.47885073691715\n",
      "74.7618507969423\n",
      "2\n",
      "not recognized\n",
      "Confidence: 74.7618507969423\n",
      "72.18500227010055\n",
      "2\n",
      "not recognized\n",
      "Confidence: 72.18500227010055\n",
      "128.60116295916828\n",
      "1\n",
      "not recognized\n",
      "Confidence: 128.60116295916828\n",
      "50.19298210501201\n",
      "2\n",
      "detected\n",
      "Recognized: Tushar, Confidence: 50.19298210501201\n",
      "79.03014897663344\n",
      "2\n",
      "not recognized\n",
      "Confidence: 79.03014897663344\n",
      "70.12567436916984\n",
      "2\n",
      "not recognized\n",
      "Confidence: 70.12567436916984\n",
      "92.11249467047391\n",
      "2\n",
      "not recognized\n",
      "Confidence: 92.11249467047391\n",
      "74.98767185202475\n",
      "2\n",
      "not recognized\n",
      "Confidence: 74.98767185202475\n",
      "71.49656582108727\n",
      "2\n",
      "not recognized\n",
      "Confidence: 71.49656582108727\n",
      "72.5290625721909\n",
      "2\n",
      "not recognized\n",
      "Confidence: 72.5290625721909\n",
      "71.2636238353383\n",
      "2\n",
      "not recognized\n",
      "Confidence: 71.2636238353383\n",
      "69.37060067243951\n",
      "2\n",
      "detected\n",
      "Recognized: Tushar, Confidence: 69.37060067243951\n",
      "70.05014543351432\n",
      "2\n",
      "not recognized\n",
      "Confidence: 70.05014543351432\n",
      "74.02481941171511\n",
      "2\n",
      "not recognized\n",
      "Confidence: 74.02481941171511\n",
      "69.49274904962607\n",
      "2\n",
      "detected\n",
      "Recognized: Tushar, Confidence: 69.49274904962607\n",
      "74.80666486195351\n",
      "2\n",
      "not recognized\n",
      "Confidence: 74.80666486195351\n",
      "70.13319410923748\n",
      "2\n",
      "not recognized\n",
      "Confidence: 70.13319410923748\n",
      "64.11374332485494\n",
      "2\n",
      "detected\n",
      "Recognized: Tushar, Confidence: 64.11374332485494\n",
      "69.38563316340905\n",
      "2\n",
      "detected\n",
      "Recognized: Tushar, Confidence: 69.38563316340905\n",
      "63.28806924803518\n",
      "2\n",
      "detected\n",
      "Recognized: Tushar, Confidence: 63.28806924803518\n",
      "62.82356689688095\n",
      "2\n",
      "detected\n",
      "Recognized: Tushar, Confidence: 62.82356689688095\n",
      "67.077142531211\n",
      "2\n",
      "detected\n",
      "Recognized: Tushar, Confidence: 67.077142531211\n",
      "65.0832082355671\n",
      "2\n",
      "detected\n",
      "Recognized: Tushar, Confidence: 65.0832082355671\n",
      "73.53235972700223\n",
      "2\n",
      "not recognized\n",
      "Confidence: 73.53235972700223\n",
      "71.0924616878155\n",
      "2\n",
      "not recognized\n",
      "Confidence: 71.0924616878155\n",
      "72.39719577277216\n",
      "2\n",
      "not recognized\n",
      "Confidence: 72.39719577277216\n",
      "69.59875551968265\n",
      "2\n",
      "detected\n",
      "Recognized: Tushar, Confidence: 69.59875551968265\n"
     ]
    }
   ],
   "source": [
    "import cv2 \n",
    "import numpy \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.chdir(r'E:/MachineLearning/Machine learning project - Copy')\n",
    "haar_file = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
    "datasets = 'data'\n",
    "\n",
    "print('Line 9')\n",
    "# Create a list of images and a list of corresponding names\n",
    "(images, labels, names, id) = ([], [], {}, 0)\n",
    "for (subdirs, dirs, files) in os.walk(datasets):\n",
    "    for subdir in dirs:\n",
    "        names[id] = subdir\n",
    "        subjectpath = os.path.join(datasets, subdir)\n",
    "        for filename in os.listdir(subjectpath):\n",
    "            path = os.path.join(subjectpath, filename)\n",
    "            label = id\n",
    "            images.append(cv2.imread(path, 0))\n",
    "            labels.append(int(label))\n",
    "        id += 1\n",
    "\n",
    "width, height = 130, 100\n",
    "\n",
    "print('Line 25')\n",
    "# Create a Numpy array \n",
    "(images, labels) = [numpy.array(lst) for lst in [images, labels]]\n",
    "\n",
    "print('Line 29')\n",
    "# Train Model\n",
    "model = cv2.face.LBPHFaceRecognizer_create()\n",
    "print('line 32')\n",
    "model.train(images, labels)\n",
    "\n",
    "print('I am Home lander')\n",
    "# Face Recognition \n",
    "face_cascade = cv2.CascadeClassifier(haar_file)\n",
    "webcam = cv2.VideoCapture(\"Video_5.mp4\")\n",
    "c = 1\n",
    "while True:\n",
    "    ret,frame = webcam.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 4)\n",
    "    for (x, y, w, h) in faces:\n",
    "        face = gray[y:y + h, x:x + w]\n",
    "        face_resize = cv2.resize(face, (width, height))\n",
    "        # Try to recognize the face\n",
    "        # Try to recognize the face\n",
    "        prediction_label, confidence = model.predict(face_resize)\n",
    "        print(confidence) \n",
    "        print(prediction_label)\n",
    "        # Display recognized label and confidence score\n",
    "        if confidence < 70:\n",
    "            print('detected')\n",
    "            print('Recognized: {}, Confidence: {}'.format(names[prediction_label], confidence))\n",
    "            cv2.putText(frame, '%s' % (names[prediction_label]), (x-10, y-10), cv2.FONT_HERSHEY_COMPLEX_SMALL, 2, (0, 255, 0))\n",
    "        else:\n",
    "            print('not recognized')\n",
    "            print('Confidence: {}'.format(confidence))\n",
    "            cv2.putText(frame, 'not recognized', (x-10, y-10), cv2.FONT_HERSHEY_PLAIN, 1, (0, 255, 0))\n",
    "\n",
    "        \n",
    "    # Display image using matplotlib\n",
    "    # print(im)\n",
    "    cv2.imshow('recognised',frame)\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == 'a':\n",
    "        break\n",
    "\n",
    "webcam.release()\n",
    "# Destroy the windows you created\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b65dd653-f9eb-44da-84e9-3b00e30b44e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detected\n",
      "Recognized: Tushar, Confidence: 18.338267550766904\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Function to load images and labels from a given directory\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    labels = []\n",
    "    names = {}\n",
    "    label_id = 0\n",
    "    for subdir in os.listdir(folder):\n",
    "        names[label_id] = subdir\n",
    "        subfolder_path = os.path.join(folder, subdir)\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            for filename in os.listdir(subfolder_path):\n",
    "                img_path = os.path.join(subfolder_path, filename)\n",
    "                img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "                if img is not None:\n",
    "                    images.append(img)\n",
    "                    labels.append(label_id)\n",
    "            label_id += 1\n",
    "    return images, labels, names\n",
    "\n",
    "# Load images and labels from the 'data' folder\n",
    "images, labels, names = load_images_from_folder('data')\n",
    "\n",
    "# Create LBPH face recognizer and train it\n",
    "recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "recognizer.train(images, np.array(labels))\n",
    "\n",
    "# Load the image to be recognized\n",
    "image_path = 'TSmile.png'\n",
    "image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Detect faces in the image\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "faces = face_cascade.detectMultiScale(image, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "# Recognize faces and draw rectangles around them\n",
    "for (x, y, w, h) in faces:\n",
    "    roi = image[y:y+h, x:x+w]\n",
    "    label_id, confidence = recognizer.predict(roi)\n",
    "    \n",
    "    if confidence < 70:\n",
    "            print('detected')\n",
    "            print('Recognized: {}, Confidence: {}'.format(names[prediction_label], confidence))\n",
    "            cv2.putText(frame, '%s' % (names[label_id]), (x-10, y-10), cv2.FONT_HERSHEY_COMPLEX_SMALL, 2, (0, 255, 0))\n",
    "    else:\n",
    "            print('not recognized')\n",
    "            print('Confidence: {}'.format(confidence))\n",
    "            cv2.putText(frame, 'not recognized', (x-10, y-10), cv2.FONT_HERSHEY_PLAIN, 1, (0, 255, 0))\n",
    "    \n",
    "\n",
    "# Display the recognized faces\n",
    "cv2.imshow('Recognized Faces', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65098e4c-2a2c-4223-a215-48bc869f65eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Models/ModelToRecogniseUnrecogniseFace.yml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae8e5372-989e-4bc2-935a-262c7b926dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model from file\n",
    "recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "recognizer.read(\"Models/ModelToRecogniseUnrecogniseFace.yml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36302119-1882-4479-8bc8-9546063f07c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
