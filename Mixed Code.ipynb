{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f617e3c7-3285-4bb7-9d70-675652e4fd9d",
   "metadata": {},
   "source": [
    "# First Getting the face recognition Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7417211e-5911-4372-aac3-2e683cce4e91",
   "metadata": {},
   "source": [
    "### A. Loading the Model of face recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df29e38c-0def-40b5-b719-086d60e47377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 298ms/step\n",
      "[[0.22735101 0.23049015 0.30723032 0.2349285 ]]\n",
      "Tushar\n",
      "0.30723032\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "[[0.22842397 0.23996438 0.2903122  0.24129948]]\n",
      "Tushar\n",
      "0.2903122\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "[[0.23339973 0.2637846  0.25715137 0.24566428]]\n",
      "Sairaj\n",
      "0.2637846\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "[[0.22926648 0.24147065 0.2927727  0.23649013]]\n",
      "Tushar\n",
      "0.2927727\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
      "[[0.22858731 0.2534371  0.27214712 0.24582839]]\n",
      "Tushar\n",
      "0.27214712\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "[[0.23250245 0.24910252 0.2787357  0.23965931]]\n",
      "Tushar\n",
      "0.2787357\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "[[0.23469993 0.25975809 0.26825058 0.23729137]]\n",
      "Tushar\n",
      "0.26825058\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "[[0.22994106 0.24296787 0.2873292  0.23976192]]\n",
      "Tushar\n",
      "0.2873292\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "[[0.23058335 0.2460545  0.28304008 0.24032205]]\n",
      "Tushar\n",
      "0.28304008\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "[[0.23207656 0.24970926 0.2768764  0.24133772]]\n",
      "Tushar\n",
      "0.2768764\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "[[0.2327395  0.25108516 0.27370086 0.24247448]]\n",
      "Tushar\n",
      "0.27370086\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "[[0.23241396 0.262176   0.25757074 0.24783933]]\n",
      "Sairaj\n",
      "0.262176\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "[[0.23224242 0.27922288 0.24407944 0.24445531]]\n",
      "Sairaj\n",
      "0.27922288\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "[[0.23321724 0.2783157  0.244128   0.24433908]]\n",
      "Sairaj\n",
      "0.2783157\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "[[0.23421246 0.2569679  0.27274036 0.23607926]]\n",
      "Tushar\n",
      "0.27274036\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "[[0.23000713 0.24580352 0.28038824 0.2438011 ]]\n",
      "Tushar\n",
      "0.28038824\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "[[0.2312466  0.27610412 0.24795908 0.24469016]]\n",
      "Sairaj\n",
      "0.27610412\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "[[0.22787993 0.2373779  0.29261702 0.24212517]]\n",
      "Tushar\n",
      "0.29261702\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "[[0.23250704 0.24964416 0.28264514 0.23520368]]\n",
      "Tushar\n",
      "0.28264514\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "[[0.23061596 0.24412811 0.28345314 0.24180283]]\n",
      "Tushar\n",
      "0.28345314\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "[[0.22887687 0.23844454 0.293602   0.23907663]]\n",
      "Tushar\n",
      "0.293602\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "import keras\n",
    "\n",
    "mp = keras.models.load_model(\"Models/Model.keras\")\n",
    "\n",
    "\n",
    "# Load the face classifier\n",
    "face_classifier = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "# Load the face map for label mapping\n",
    "with open(\"ResultsMap.pkl\", 'rb') as fileReadStream:\n",
    "    ResultMap = pickle.load(fileReadStream)\n",
    "\n",
    "# Initialize the webcam\n",
    "webcam = cv2.VideoCapture(\"Video_5.mp4\")\n",
    "\n",
    "# Function to recognize faces\n",
    "def recognize_faces(frame):\n",
    "    # Convert the frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in the frame\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Extract the face ROI\n",
    "        face_roi = frame[y:y+h, x:x+w]\n",
    "\n",
    "        # Resize the face ROI to match the input size of the model\n",
    "        resized_face = cv2.resize(face_roi, (300, 300))\n",
    "\n",
    "        # Preprocess the face (normalize pixel values)\n",
    "        normalized_face = resized_face / 255.0\n",
    "\n",
    "        # Reshape the face for the model input (add batch dimension)\n",
    "        reshaped_face = np.expand_dims(normalized_face, axis=0)\n",
    "\n",
    "        # Perform prediction using the model\n",
    "        prediction = mp.predict(reshaped_face)\n",
    "        predicted_label = ResultMap[np.argmax(prediction)]\n",
    "        print(prediction)\n",
    "        print(predicted_label)\n",
    "        print(np.max(prediction))\n",
    "\n",
    "        if np.max(prediction) < 0.25: \n",
    "            predicted_label = 'Unknown' \n",
    "\n",
    "        # Draw the bounding box and label on the frame\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        cv2.putText(frame, predicted_label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n",
    "\n",
    "    return frame\n",
    "\n",
    "# Main loop to capture frames from the webcam\n",
    "while True:\n",
    "    # Read a frame from the webcam\n",
    "    ret, frame = webcam.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Perform face recognition on the frame\n",
    "    frame = recognize_faces(frame)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Face Recognition', frame)\n",
    "\n",
    "    # Check for 'q' key to quit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close all OpenCV windows\n",
    "webcam.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bffa79-4b8b-480c-85ee-e6494d46333f",
   "metadata": {},
   "source": [
    "## B. Loading the model of the action recongition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a52ecc8-5b61-45e3-9017-e84da213d532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Entered1\n",
      "[[[209 203 184]\n",
      "  [209 203 184]\n",
      "  [210 204 185]\n",
      "  ...\n",
      "  [224 215 205]\n",
      "  [224 215 205]\n",
      "  [224 215 205]]\n",
      "\n",
      " [[209 203 184]\n",
      "  [210 204 185]\n",
      "  [210 204 185]\n",
      "  ...\n",
      "  [224 215 205]\n",
      "  [224 215 205]\n",
      "  [224 215 205]]\n",
      "\n",
      " [[210 204 185]\n",
      "  [210 204 185]\n",
      "  [210 204 185]\n",
      "  ...\n",
      "  [224 215 205]\n",
      "  [221 212 202]\n",
      "  [221 212 202]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[185 183 189]\n",
      "  [180 178 184]\n",
      "  [153 153 159]\n",
      "  ...\n",
      "  [188 193 196]\n",
      "  [195 201 206]\n",
      "  [197 203 208]]\n",
      "\n",
      " [[177 175 181]\n",
      "  [178 176 182]\n",
      "  [165 165 171]\n",
      "  ...\n",
      "  [178 183 186]\n",
      "  [194 200 205]\n",
      "  [196 202 207]]\n",
      "\n",
      " [[172 170 176]\n",
      "  [184 182 188]\n",
      "  [189 189 195]\n",
      "  ...\n",
      "  [175 180 183]\n",
      "  [182 188 193]\n",
      "  [184 190 195]]]\n",
      "landmarks\n",
      "x: 0.54023635\n",
      "y: 0.90556383\n",
      "z: 0.2435935\n",
      "visibility: 0.96696174\n",
      "\n",
      "     NOSE_X   NOSE_Y   NOSE_Z    NOSE_V  LEFT_EYE_INNER_X  LEFT_EYE_INNER_Y  \\\n",
      "0 -0.216069 -0.49518 -0.54368  0.999696         -0.209308         -0.530185   \n",
      "\n",
      "   LEFT_EYE_INNER_Z  LEFT_EYE_INNER_V  LEFT_EYE_X  LEFT_EYE_Y  ...  \\\n",
      "0         -0.577265          0.999476   -0.204404   -0.532712  ...   \n",
      "\n",
      "   RIGHT_HEEL_Z  RIGHT_HEEL_V  LEFT_FOOT_INDEX_X  LEFT_FOOT_INDEX_Y  \\\n",
      "0      0.502936      0.933512           0.279738           0.488419   \n",
      "\n",
      "   LEFT_FOOT_INDEX_Z  LEFT_FOOT_INDEX_V  RIGHT_FOOT_INDEX_X  \\\n",
      "0           -0.05191           0.992711           -0.049632   \n",
      "\n",
      "   RIGHT_FOOT_INDEX_Y  RIGHT_FOOT_INDEX_Z  RIGHT_FOOT_INDEX_V  \n",
      "0            0.656172            0.413901            0.966962  \n",
      "\n",
      "[1 rows x 132 columns]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step\n",
      "predicted\n",
      "predictions:  [9.3474030e-01 1.9174205e-02 4.6054941e-02 3.0652267e-05]\n",
      "predicted Pose Class:  Running\n",
      "[INFO] Output Image Saved in ImageOutput/Running2.jpg\n",
      "[INFO] Inference on Test Image is Ended...\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers, Sequential\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os \n",
    "\n",
    "\n",
    "# ap = argparse.ArgumentParser()\n",
    "# ap.add_argument(\"-m\", \"--model\", type=str, required=True,\n",
    "#                 help=\"path to saved .h5 model, eg: dir/model.h5\")\n",
    "# ap.add_argument(\"-c\", \"--conf\", type=float, required=True,\n",
    "#                 help=\"min prediction conf to detect pose class (0<conf<1)\")\n",
    "# ap.add_argument(\"-i\", \"--source\", type=str, required=True,\n",
    "#                 help=\"path to sample image\")\n",
    "# ap.add_argument(\"--save\", action='store_true',\n",
    "#                 help=\"Save video\")\n",
    "\n",
    "# args = vars(ap.parse_args())\n",
    "source = \"Running2.jpeg\"\n",
    "path_saved_model = \"Models/Action Recognition/model.keras\"\n",
    "threshold = 0.2\n",
    "save = \"Output\"\n",
    "print('Hello')\n",
    "##############\n",
    "torso_size_multiplier = 2.5\n",
    "n_landmarks = 33\n",
    "n_dimensions = 3\n",
    "landmark_names = [\n",
    "    'nose',\n",
    "    'left_eye_inner', 'left_eye', 'left_eye_outer',\n",
    "    'right_eye_inner', 'right_eye', 'right_eye_outer',\n",
    "    'left_ear', 'right_ear',\n",
    "    'mouth_left', 'mouth_right',\n",
    "    'left_shoulder', 'right_shoulder',\n",
    "    'left_elbow', 'right_elbow',\n",
    "    'left_wrist', 'right_wrist',\n",
    "    'left_pinky_1', 'right_pinky_1',\n",
    "    'left_index_1', 'right_index_1',\n",
    "    'left_thumb_2', 'right_thumb_2',\n",
    "    'left_hip', 'right_hip',\n",
    "    'left_knee', 'right_knee',\n",
    "    'left_ankle', 'right_ankle',\n",
    "    'left_heel', 'right_heel',\n",
    "    'left_foot_index', 'right_foot_index',\n",
    "]\n",
    "class_names = [\n",
    "    'Running','Standing','Walking','Waving'\n",
    "]\n",
    "##############\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "col_names = []\n",
    "for i in range(n_landmarks):\n",
    "    name = mp_pose.PoseLandmark(i).name\n",
    "    name_x = name + '_X'\n",
    "    name_y = name + '_Y'\n",
    "    name_z = name + '_Z'\n",
    "    name_v = name + '_V'\n",
    "    col_names.append(name_x)\n",
    "    col_names.append(name_y)\n",
    "    col_names.append(name_z)\n",
    "    col_names.append(name_v)\n",
    "\n",
    "# Load saved model\n",
    "model = load_model(path_saved_model, compile=True)\n",
    "\n",
    "if source.endswith(('.jpg', '.jpeg', '.png')):\n",
    "    path_to_img = source\n",
    "    print('Entered1')\n",
    "    # Load sample Image\n",
    "    \n",
    "    img = cv2.imread(path_to_img)\n",
    "    print(img)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    result = pose.process(img_rgb)\n",
    "    if result.pose_landmarks:\n",
    "        lm_list = []\n",
    "        for landmarks in result.pose_landmarks.landmark:\n",
    "            # Preprocessing\n",
    "            max_distance = 0\n",
    "            lm_list.append(landmarks)\n",
    "        print('landmarks')\n",
    "        print(landmarks)\n",
    "        center_x = (lm_list[landmark_names.index('right_hip')].x +\n",
    "                    lm_list[landmark_names.index('left_hip')].x)*0.5\n",
    "        center_y = (lm_list[landmark_names.index('right_hip')].y +\n",
    "                    lm_list[landmark_names.index('left_hip')].y)*0.5\n",
    "\n",
    "        shoulders_x = (lm_list[landmark_names.index('right_shoulder')].x +\n",
    "                       lm_list[landmark_names.index('left_shoulder')].x)*0.5\n",
    "        shoulders_y = (lm_list[landmark_names.index('right_shoulder')].y +\n",
    "                       lm_list[landmark_names.index('left_shoulder')].y)*0.5\n",
    "\n",
    "        for lm in lm_list:\n",
    "            distance = math.sqrt((lm.x - center_x)**2 + (lm.y - center_y)**2)\n",
    "            if(distance > max_distance):\n",
    "                max_distance = distance\n",
    "        torso_size = math.sqrt((shoulders_x - center_x) **\n",
    "                               2 + (shoulders_y - center_y)**2)\n",
    "        max_distance = max(torso_size*torso_size_multiplier, max_distance)\n",
    "\n",
    "        pre_lm = list(np.array([[(landmark.x-center_x)/max_distance, (landmark.y-center_y)/max_distance,\n",
    "                                 landmark.z/max_distance, landmark.visibility] for landmark in lm_list]).flatten())\n",
    "        data = pd.DataFrame([pre_lm], columns=col_names)\n",
    "        print(data)\n",
    "        predict = model.predict(data)[0]\n",
    "        print('predicted')\n",
    "        if max(predict) > threshold:\n",
    "            pose_class = class_names[predict.argmax()]\n",
    "            print('predictions: ', predict)\n",
    "            print('predicted Pose Class: ', pose_class)\n",
    "        else:\n",
    "            pose_class = 'Unknown Pose'\n",
    "            print('[INFO] Predictions is below given Confidence!!')\n",
    "\n",
    "    # Show Result\n",
    "    img = cv2.putText(\n",
    "        img, f'{class_names[predict.argmax()]}',\n",
    "        (40, 50), cv2.FONT_HERSHEY_PLAIN,\n",
    "        2, (255, 0, 255), 2\n",
    "    )\n",
    "\n",
    "    if save:\n",
    "        os.makedirs('ImageOutput', exist_ok=True)\n",
    "        img_full_name = os.path.split(path_to_img)[1]\n",
    "        img_name = os.path.splitext(img_full_name)[0]\n",
    "        path_to_save_img = f'ImageOutput/{img_name}.jpg'\n",
    "        cv2.imwrite(f'{path_to_save_img}', img)\n",
    "        print(f'[INFO] Output Image Saved in {path_to_save_img}')\n",
    "\n",
    "    cv2.imshow('Output Image', img)\n",
    "    if cv2.waitKey(0) & 0xFF == ord('q'):\n",
    "        cv2.destroyAllWindows()\n",
    "    print('[INFO] Inference on Test Image is Ended...')\n",
    "\n",
    "else:\n",
    "    # Web-cam\n",
    "    if source.isnumeric():\n",
    "        source = int(source)\n",
    "\n",
    "    cap = cv2.VideoCapture(source)\n",
    "    source_width = int(cap.get(3))\n",
    "    source_height = int(cap.get(4))\n",
    "\n",
    "    # Write Video\n",
    "    if save:\n",
    "        out_video = cv2.VideoWriter('output.avi', \n",
    "                            cv2.VideoWriter_fourcc(*'MJPG'),\n",
    "                            10, (source_width, source_height))\n",
    "\n",
    "    while True:\n",
    "        success, img = cap.read()\n",
    "        if not success:\n",
    "            print('[ERROR] Failed to Read Video feed')\n",
    "            break\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        result = pose.process(img_rgb)\n",
    "\n",
    "        if result.pose_landmarks:\n",
    "            lm_list = []\n",
    "            for landmarks in result.pose_landmarks.landmark:\n",
    "                # Preprocessing\n",
    "                max_distance = 0\n",
    "                lm_list.append(landmarks)\n",
    "            center_x = (lm_list[landmark_names.index('right_hip')].x +\n",
    "                        lm_list[landmark_names.index('left_hip')].x)*0.5\n",
    "            center_y = (lm_list[landmark_names.index('right_hip')].y +\n",
    "                        lm_list[landmark_names.index('left_hip')].y)*0.5\n",
    "\n",
    "            shoulders_x = (lm_list[landmark_names.index('right_shoulder')].x +\n",
    "                           lm_list[landmark_names.index('left_shoulder')].x)*0.5\n",
    "            shoulders_y = (lm_list[landmark_names.index('right_shoulder')].y +\n",
    "                           lm_list[landmark_names.index('left_shoulder')].y)*0.5\n",
    "\n",
    "            for lm in lm_list:\n",
    "                distance = math.sqrt((lm.x - center_x) **\n",
    "                                     2 + (lm.y - center_y)**2)\n",
    "                if(distance > max_distance):\n",
    "                    max_distance = distance\n",
    "            torso_size = math.sqrt((shoulders_x - center_x) **\n",
    "                                   2 + (shoulders_y - center_y)**2)\n",
    "            max_distance = max(torso_size*torso_size_multiplier, max_distance)\n",
    "\n",
    "            pre_lm = list(np.array([[(landmark.x-center_x)/max_distance, (landmark.y-center_y)/max_distance,\n",
    "                                     landmark.z/max_distance, landmark.visibility] for landmark in lm_list]).flatten())\n",
    "            data = pd.DataFrame([pre_lm], columns=col_names)\n",
    "            # print(data[2])\n",
    "            predict = model.predict(data)[0]\n",
    "            if max(predict) > threshold:\n",
    "                pose_class = class_names[predict.argmax()]\n",
    "                print('predictions: ', predict)\n",
    "                print('predicted Pose Class: ', pose_class)\n",
    "            else:\n",
    "                pose_class = 'Unknown Pose'\n",
    "                print('[INFO] Predictions is below given Confidence!!')\n",
    "\n",
    "            # Show Result\n",
    "            img = cv2.putText(\n",
    "                img, f'{pose_class}',\n",
    "                (40, 50), cv2.FONT_HERSHEY_PLAIN,\n",
    "                2, (255, 0, 255), 2\n",
    "            )\n",
    "        # Write Video\n",
    "        if save:\n",
    "            out_video.write(img)\n",
    "\n",
    "        cv2.imshow('Output Image', img)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    if save:\n",
    "        out_video.release()\n",
    "        print(\"[INFO] Out video Saved as 'output.avi'\")\n",
    "    cv2.destroyAllWindows()\n",
    "    print('[INFO] Inference on Videostream is Ended...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9105beb8-c397-4bdf-a91b-e072ed3d44b1",
   "metadata": {},
   "source": [
    "## C. Intrusion Detection code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d213ee1-2d54-4701-9ad0-f4c410fd0b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries succesfully imported\n"
     ]
    }
   ],
   "source": [
    "# Import the required libraries\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import datetime\n",
    "from collections import deque\n",
    "from twilio.rest import Client \n",
    "import random\n",
    "import heapq\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import requests\n",
    "\n",
    "\n",
    "print(\"Libraries succesfully imported\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22345f33-69d1-463c-b2ef-f95f541c6160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no error in functions\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def is_person_present(frame, thresh=1100):\n",
    "    kernel = None\n",
    "    global foog\n",
    "    \n",
    "    # Apply background subtraction\n",
    "    fgmask = foog.apply(frame)\n",
    "\n",
    "    # Get rid of the shadows\n",
    "    ret, fgmask = cv2.threshold(fgmask, 250, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Apply some morphological operations to make sure you have a good mask\n",
    "    fgmask = cv2.dilate(fgmask, kernel, iterations=4)\n",
    "\n",
    "    # Detect contours in the frame\n",
    "    contours, hierarchy = cv2.findContours(fgmask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "     \n",
    "    # Check if there was a contour and the area is somewhat higher than some threshold so we know it's a person and not noise\n",
    "    if contours and cv2.contourArea(max(contours, key=cv2.contourArea)) > thresh:\n",
    "            \n",
    "        # Get the max contour\n",
    "        cnt = max(contours, key=cv2.contourArea)\n",
    "\n",
    "        # Draw a bounding box around the person and label it as person detected\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
    "        cv2.putText(frame, 'Person Detected', (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "        \n",
    "        return True, frame\n",
    "        \n",
    "        \n",
    "    # Otherwise report there was no one present\n",
    "    else:\n",
    "        return False, frame\n",
    "\n",
    "\n",
    "def send_message(body, info_dict):\n",
    "\n",
    "    # Your Account SID from twilio.com/console\n",
    "    account_sid = info_dict['account_sid']\n",
    "\n",
    "    # Your Auth Token from twilio.com/console\n",
    "    auth_token  = info_dict['auth_token']\n",
    "\n",
    "\n",
    "    client = Client(account_sid, auth_token)\n",
    "\n",
    "    message = client.messages.create(to=info_dict['your_num'], from_=info_dict['trial_num'], body=body)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_percentage_change(frame1, frame2):\n",
    "    # Convert frames to grayscale\n",
    "    gray_frame1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "    gray_frame2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Calculate absolute difference between the two frames\n",
    "    abs_diff = cv2.absdiff(gray_frame1, gray_frame2)\n",
    "\n",
    "    # Calculate percentage change\n",
    "    percentage_change = (np.count_nonzero(abs_diff) / abs_diff.size) * 100\n",
    "\n",
    "    return percentage_change\n",
    "\n",
    "\n",
    "def save_frame(frame_heap): \n",
    "    frame_index = 0\n",
    "    while frame_heap:\n",
    "        # Pop the frame with the maximum percentage change (negated)\n",
    "        neg_change_percentage, frame , date = heapq.heappop(frame_heap)\n",
    "        change_percentage = -neg_change_percentage\n",
    "        file_path = 'DetectedPerson'\n",
    "        # Save the frame to disk\n",
    "        cv2.imwrite(f'{file_path}/{date}.jpg', frame)\n",
    "        frame_index += 1\n",
    "    print(\"saved frame succesfully\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "print('no error in functions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51e8392-2f9e-4cbc-ab1b-56be340c2dad",
   "metadata": {},
   "source": [
    "## Final Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7261b3c7-6383-4e39-b6cd-9ffae621c74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Window normal so we can resize it\n",
    "cv2.namedWindow('frame', cv2.WINDOW_NORMAL)\n",
    "\n",
    "# This is a test video\n",
    "cap = cv2.VideoCapture('sample_video1.mp4')\n",
    "\n",
    "# Read the video stream from the camera\n",
    "# cap = cv2.VideoCapture('http://192.168.43.1:8080/video')\n",
    "\n",
    "# Get width and height of the frame\n",
    "width = int(cap.get(3))\n",
    "height = int(cap.get(4))\n",
    "\n",
    "# Read and store the credentials information in a dict\n",
    "with open('credential.txt', 'r') as myfile:\n",
    "  data = myfile.read()\n",
    "\n",
    "info_dict = eval(data)\n",
    "\n",
    "# Initialize the background Subtractor\n",
    "foog = cv2.createBackgroundSubtractorMOG2(detectShadows=True, varThreshold=100, history=2000)\n",
    "\n",
    "# Status is True when person is present and False when the person is not present.\n",
    "status = False\n",
    "\n",
    "# After the person disappears from view, wait at least 7 seconds before making the status False\n",
    "patience = 0.099\n",
    "\n",
    "# We don't consider an initial detection unless it's detected 15 times, this gets rid of false positives\n",
    "detection_thresh = 5\n",
    "\n",
    "# Initial time for calculating if patience time is up\n",
    "initial_time = None\n",
    "\n",
    "\n",
    "\n",
    "# We are creating a deque object of length detection_thresh and will store individual detection statuses here\n",
    "de = deque([False] * 10, maxlen=10)\n",
    "\n",
    "# Initialize an empty heap to store frames along with their negated percentage change\n",
    "frame_heap = []\n",
    "\n",
    "\n",
    "# Initialize these variables for calculating FPS\n",
    "fps = 0 \n",
    "frame_counter = 0\n",
    "start_time = time.time()\n",
    "counter = 0\n",
    "\n",
    "ret,prevframe = cap.read()\n",
    "initial_frame = prevframe\n",
    "recognisedName = \"\"\n",
    "sent  = False\n",
    "\n",
    "while True:\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break \n",
    "    \n",
    "            \n",
    "    # This function will return a boolean variable telling if someone was present or not, it will also draw boxes if it \n",
    "    # finds someone\n",
    "    detected, annotated_image = is_person_present(frame)  \n",
    "    # print(recognisedName)\n",
    "    if detected and len(recognisedName) == 0: \n",
    "        # result = recogniseFromFace(frame)  \n",
    "        result = ''\n",
    "        if result is not None: \n",
    "            result = ''\n",
    "    # Register the current detection status on our deque object\n",
    "    de.appendleft(detected)\n",
    "    \n",
    "    # If we have consecutively detected a person 15 times then we are sure that someone is present    \n",
    "    # We also make this is the first time that this person has been detected so we only initialize the videowriter once\n",
    "    if sum(de) == detection_thresh and not status:                       \n",
    "            status = True\n",
    "            entry_time = datetime.datetime.now().strftime(\"%A, %I-%M-%S %p %d %B %Y\")\n",
    "            # out = cv2.VideoWriter('outputs/{}.mp4'.format(entry_time), cv2.VideoWriter_fourcc(*'XVID'), 15.0, (width, height))\n",
    "\n",
    "    # If status is True but the person is not in the current frame\n",
    "    if status and not detected:\n",
    "        \n",
    "        # Restart the patience timer only if the person has not been detected for a few frames so we are sure it wasn't a \n",
    "        # False positive\n",
    "        print('i reached deepest of the darkest point')\n",
    "        if sum(de) > (detection_thresh/2): \n",
    "            if initial_time is None:\n",
    "                print('initial time as been setted to',initial_time)\n",
    "                initial_time = time.time()\n",
    "                \n",
    "            \n",
    "            elif initial_time is not None:        \n",
    "                print('now the initial time is not null beacuse current time is ',initial_time)\n",
    "                print(time.time() - initial_time)\n",
    "                # If the patience has run out and the person is still not detected then set the status to False\n",
    "                # Also save the video by releasing the video writer and send a text message.\n",
    "                if  time.time() - initial_time >= patience:\n",
    "                    status = False\n",
    "                    print(\"inside the loop\")\n",
    "                    exit_time = datetime.datetime.now().strftime(\"%A, %I:%M:%S %p %d %B %Y\")\n",
    "                    # out.release()\n",
    "                    # print(out)\n",
    "                    initial_time = None\n",
    "                    de = deque([False] * len(de))\n",
    "                    body = \"Alert: \\nA Person Entered the Room at {} \\nLeft the room at {}\".format(entry_time, exit_time) \n",
    "                    if len(recognisedName) > 0: \n",
    "                        body = \"Alert: {} Has entered in your Room at {} \\n Left the room at {}\".format(recognisedName,entry_time,exit_time)\n",
    "                    print('message has been sent')\n",
    "                    send_message(body, info_dict)\n",
    "                    frame_index = 0\n",
    "                    cv2.destroyWindow(\"frame\")\n",
    "                    while frame_heap:\n",
    "                        # Pop the frame with the maximum percentage change (negated)\n",
    "                        neg_change_percentage, frame , date = heapq.heappop(frame_heap)\n",
    "                        change_percentage = -neg_change_percentage\n",
    "                        file_path = 'DetectedPerson'\n",
    "                        \n",
    "                        # Save the frame to disk\n",
    "                        cv2.imwrite(f'{file_path}/{date}.jpg', frame)\n",
    "                        frame_index += 1\n",
    "                    print(\"saved frame succesfully\")\n",
    "                    continue\n",
    "                    \n",
    "                \n",
    "    \n",
    "    # If a significant amount of detections (more than half of detection_thresh) has occurred then we reset the Initial Time.\n",
    "    elif status and sum(de) > (detection_thresh/2):\n",
    "        change_percentage = calculate_percentage_change(prevframe, frame)\n",
    "        change_initial_change = calculate_percentage_change(initial_frame,frame)\n",
    "        # print(change_percentage)\n",
    "        if change_percentage > 95 and sent == False: \n",
    "            sent = True\n",
    "            body = \"Alert: \\n Some one has changed the camera configuration at:{}\".format(datetime.datetime.now().strftime(\"%A, %I-%M-%S %p %d %B %Y\")) \n",
    "            send_message(body,info_dict)\n",
    "    \n",
    "        if change_initial_change > 60: \n",
    "            heapq.heappush(frame_heap, (-change_percentage, frame,datetime.datetime.now().strftime(\"%A, %I-%M-%S %p %d %B %Y\")))\n",
    "        prevframe = frame\n",
    "        initial_time = None\n",
    "    \n",
    "    # Get the current time in the required format\n",
    "    current_time = datetime.datetime.now().strftime(\"%A, %I:%M:%S %p %d %B %Y\")\n",
    "\n",
    "    # Display the FPS\n",
    "    cv2.putText(annotated_image, 'FPS: {:.2f}'.format(fps), (510, 450), cv2.FONT_HERSHEY_COMPLEX, 0.6, (255, 40, 155), 2)\n",
    "    \n",
    "    # Display Time\n",
    "    cv2.putText(annotated_image, current_time, (310, 20), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 255), 1)    \n",
    "    \n",
    "    # Display the Room Status\n",
    "    cv2.putText(annotated_image, 'Room Occupied: {}'.format(str(status)), (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, \n",
    "                (200, 10, 150), 2)\n",
    "\n",
    "    # Show the patience Value\n",
    "    if initial_time is None:\n",
    "        text = 'Patience: {}'.format(patience)\n",
    "    else: \n",
    "        text = 'Patience: {:.2f}'.format(max(0, patience - (time.time() - initial_time)))\n",
    "        \n",
    "    cv2.putText(annotated_image, text, (10, 450), cv2.FONT_HERSHEY_COMPLEX, 0.6, (255, 40, 155), 2)   \n",
    "\n",
    " \n",
    "    # Show the Frame\n",
    "    cv2.imshow('frame', frame)\n",
    "    \n",
    "    # Calculate the Average FPS\n",
    "    frame_counter += 1\n",
    "    fps = (frame_counter / (time.time() - start_time))\n",
    "    \n",
    "    \n",
    "    # Exit if q is pressed.\n",
    "    if cv2.waitKey(30) == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "# Release Capture and destroy windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d4dc20-9b83-48d4-a5e9-add1b3fc7ae8",
   "metadata": {},
   "source": [
    "## D. Final Code Of ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b997d7c9-e00e-4ed3-8f62-37b87886134f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries succesfully imported\n",
      "Hello\n",
      "no error in functions\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from collections import Counter\n",
    "import keras\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import datetime\n",
    "from collections import deque\n",
    "from twilio.rest import Client \n",
    "import random\n",
    "import heapq\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import requests\n",
    "import keras\n",
    "from keras import layers, Sequential\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "import mediapipe as mp\n",
    "import glob\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Libraries succesfully imported\") \n",
    "\n",
    "mp1 = keras.models.load_model(\"Models/Model.keras\")\n",
    "\n",
    "\n",
    "path_saved_model = \"Models/Action Recognition/model.keras\"\n",
    "threshold = 0.2\n",
    "save = \"Output\"\n",
    "print('Hello')\n",
    "##############\n",
    "torso_size_multiplier = 2.5\n",
    "n_landmarks = 33\n",
    "n_dimensions = 3\n",
    "landmark_names = [\n",
    "    'nose',\n",
    "    'left_eye_inner', 'left_eye', 'left_eye_outer',\n",
    "    'right_eye_inner', 'right_eye', 'right_eye_outer',\n",
    "    'left_ear', 'right_ear',\n",
    "    'mouth_left', 'mouth_right',\n",
    "    'left_shoulder', 'right_shoulder',\n",
    "    'left_elbow', 'right_elbow',\n",
    "    'left_wrist', 'right_wrist',\n",
    "    'left_pinky_1', 'right_pinky_1',\n",
    "    'left_index_1', 'right_index_1',\n",
    "    'left_thumb_2', 'right_thumb_2',\n",
    "    'left_hip', 'right_hip',\n",
    "    'left_knee', 'right_knee',\n",
    "    'left_ankle', 'right_ankle',\n",
    "    'left_heel', 'right_heel',\n",
    "    'left_foot_index', 'right_foot_index',\n",
    "]\n",
    "class_names = [\n",
    "    'Running','Standing','Walking','Waving'\n",
    "]\n",
    "\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "col_names = []\n",
    "for i in range(n_landmarks):\n",
    "    name = mp_pose.PoseLandmark(i).name\n",
    "    name_x = name + '_X'\n",
    "    name_y = name + '_Y'\n",
    "    name_z = name + '_Z'\n",
    "    name_v = name + '_V'\n",
    "    col_names.append(name_x)\n",
    "    col_names.append(name_y)\n",
    "    col_names.append(name_z)\n",
    "    col_names.append(name_v)\n",
    "\n",
    "# Load saved model\n",
    "model = load_model(path_saved_model, compile=True)\n",
    "\n",
    "\n",
    "# Load the face classifier\n",
    "face_classifier = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "# Load the face map for label mapping\n",
    "with open(\"ResultsMap.pkl\", 'rb') as fileReadStream:\n",
    "    ResultMap = pickle.load(fileReadStream)\n",
    "\n",
    "\n",
    "\n",
    "# Function to recognize faces\n",
    "def recognize_faces(frame):\n",
    "    # Convert the frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in the frame\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "    predicted_label = ''\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Extract the face ROI\n",
    "        face_roi = frame[y:y+h, x:x+w]\n",
    "\n",
    "        # Resize the face ROI to match the input size of the model\n",
    "        resized_face = cv2.resize(face_roi, (300, 300))\n",
    "\n",
    "        # Preprocess the face (normalize pixel values)\n",
    "        normalized_face = resized_face / 255.0\n",
    "\n",
    "        # Reshape the face for the model input (add batch dimension)\n",
    "        reshaped_face = np.expand_dims(normalized_face, axis=0)\n",
    "\n",
    "        # Perform prediction using the model\n",
    "        prediction = mp1.predict(reshaped_face)\n",
    "        predicted_label = ResultMap[np.argmax(prediction)]\n",
    "        print(prediction)\n",
    "        print(predicted_label)\n",
    "        print(np.max(prediction))\n",
    "\n",
    "        if np.max(prediction) < 0.25: \n",
    "            predicted_label = 'Unknown' \n",
    "\n",
    "        # Draw the bounding box and label on the frame\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        cv2.putText(frame, predicted_label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n",
    "    return predicted_label , frame\n",
    "\n",
    "\n",
    "def recognisize_pose(frame): \n",
    "    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = pose.process(img_rgb)\n",
    "    activity = ''\n",
    "    if result.pose_landmarks:\n",
    "        lm_list = []\n",
    "        for landmarks in result.pose_landmarks.landmark:\n",
    "            # Preprocessing\n",
    "            max_distance = 0\n",
    "            lm_list.append(landmarks)\n",
    "        # print('landmarks')\n",
    "        # print(landmarks)\n",
    "        center_x = (lm_list[landmark_names.index('right_hip')].x +\n",
    "                    lm_list[landmark_names.index('left_hip')].x)*0.5\n",
    "        center_y = (lm_list[landmark_names.index('right_hip')].y +\n",
    "                    lm_list[landmark_names.index('left_hip')].y)*0.5\n",
    "\n",
    "        shoulders_x = (lm_list[landmark_names.index('right_shoulder')].x +\n",
    "                       lm_list[landmark_names.index('left_shoulder')].x)*0.5\n",
    "        shoulders_y = (lm_list[landmark_names.index('right_shoulder')].y +\n",
    "                       lm_list[landmark_names.index('left_shoulder')].y)*0.5\n",
    "\n",
    "        for lm in lm_list:\n",
    "            distance = math.sqrt((lm.x - center_x)**2 + (lm.y - center_y)**2)\n",
    "            if(distance > max_distance):\n",
    "                max_distance = distance\n",
    "        torso_size = math.sqrt((shoulders_x - center_x) **\n",
    "                               2 + (shoulders_y - center_y)**2)\n",
    "        max_distance = max(torso_size*torso_size_multiplier, max_distance)\n",
    "\n",
    "        pre_lm = list(np.array([[(landmark.x-center_x)/max_distance, (landmark.y-center_y)/max_distance,\n",
    "                                 landmark.z/max_distance, landmark.visibility] for landmark in lm_list]).flatten())\n",
    "        data = pd.DataFrame([pre_lm], columns=col_names)\n",
    "        # print(data)\n",
    "        predict = model.predict(data)[0]\n",
    "        print('predicted')\n",
    "        activity = class_names[predict.argmax()]\n",
    "        if max(predict) > threshold:\n",
    "            pose_class = class_names[predict.argmax()]\n",
    "            print('predictions: ', predict)\n",
    "            print('predicted Pose Class: ', pose_class)\n",
    "        else:\n",
    "            pose_class = 'Unknown Pose'\n",
    "            print('[INFO] Predictions is below given Confidence!!')\n",
    "\n",
    "        frame = cv2.putText(\n",
    "                frame, f'{class_names[predict.argmax()]}',\n",
    "                (40, 50), cv2.FONT_HERSHEY_PLAIN,\n",
    "                2, (255, 0, 255), 2\n",
    "        )\n",
    "    \n",
    "    print('[INFO] Inference on Test Image is Ended...')\n",
    "    return activity \n",
    "\n",
    "\n",
    "\n",
    "def is_person_present(frame, thresh=1100):\n",
    "    kernel = None\n",
    "    global foog\n",
    "    \n",
    "    # Apply background subtraction\n",
    "    fgmask = foog.apply(frame)\n",
    "\n",
    "    # Get rid of the shadows\n",
    "    ret, fgmask = cv2.threshold(fgmask, 250, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Apply some morphological operations to make sure you have a good mask\n",
    "    fgmask = cv2.dilate(fgmask, kernel, iterations=4)\n",
    "\n",
    "    # Detect contours in the frame\n",
    "    contours, hierarchy = cv2.findContours(fgmask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "     \n",
    "    # Check if there was a contour and the area is somewhat higher than some threshold so we know it's a person and not noise\n",
    "    if contours and cv2.contourArea(max(contours, key=cv2.contourArea)) > thresh:\n",
    "            \n",
    "        # Get the max contour\n",
    "        cnt = max(contours, key=cv2.contourArea)\n",
    "\n",
    "        # Draw a bounding box around the person and label it as person detected\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
    "        cv2.putText(frame, 'Person Detected', (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "        \n",
    "        return True, frame\n",
    "        \n",
    "        \n",
    "    # Otherwise report there was no one present\n",
    "    else:\n",
    "        return False, frame\n",
    "\n",
    "\n",
    "def send_message(body, info_dict):\n",
    "\n",
    "    # Your Account SID from twilio.com/console\n",
    "    account_sid = info_dict['account_sid']\n",
    "\n",
    "    # Your Auth Token from twilio.com/console\n",
    "    auth_token  = info_dict['auth_token']\n",
    "\n",
    "\n",
    "    client = Client(account_sid, auth_token)\n",
    "\n",
    "    message = client.messages.create(to=info_dict['your_num'], from_=info_dict['trial_num'], body=body)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_percentage_change(frame1, frame2):\n",
    "    # Convert frames to grayscale\n",
    "    gray_frame1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "    gray_frame2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Calculate absolute difference between the two frames\n",
    "    abs_diff = cv2.absdiff(gray_frame1, gray_frame2)\n",
    "\n",
    "    # Calculate percentage change\n",
    "    percentage_change = (np.count_nonzero(abs_diff) / abs_diff.size) * 100\n",
    "\n",
    "    return percentage_change\n",
    "\n",
    "\n",
    "def save_frame(frame_heap): \n",
    "    frame_index = 0\n",
    "    while frame_heap:\n",
    "        # Pop the frame with the maximum percentage change (negated)\n",
    "        neg_change_percentage, frame , date = heapq.heappop(frame_heap)\n",
    "        change_percentage = -neg_change_percentage\n",
    "        file_path = 'DetectedPerson'\n",
    "        # Save the frame to disk\n",
    "        cv2.imwrite(f'{file_path}/{date}.jpg', frame)\n",
    "        frame_index += 1\n",
    "    print(\"saved frame succesfully\")\n",
    "\n",
    "print('no error in functions')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54c0e123-96a4-460a-ad5b-23ef83416578",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "i reached deepest of the darkest point\n",
      "initial time as been setted to None\n",
      "i reached deepest of the darkest point\n",
      "now the initial time is not null beacuse current time is  1712647887.544413\n",
      "0.12106037139892578\n",
      "inside the loop\n",
      "message has been sent\n",
      "saved frame succesfully\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "predicted\n",
      "predictions:  [0.18607916 0.08591764 0.21239041 0.5156127 ]\n",
      "predicted Pose Class:  Waving\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "predicted\n",
      "predictions:  [0.4636249  0.39266002 0.02966832 0.11404681]\n",
      "predicted Pose Class:  Running\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "predicted\n",
      "predictions:  [0.38645476 0.35746908 0.03950598 0.21657024]\n",
      "predicted Pose Class:  Running\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "predicted\n",
      "predictions:  [0.04987476 0.8221554  0.06087415 0.0670957 ]\n",
      "predicted Pose Class:  Standing\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "predicted\n",
      "predictions:  [0.30889547 0.16371651 0.40281114 0.12457694]\n",
      "predicted Pose Class:  Walking\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "predicted\n",
      "predictions:  [6.1313492e-01 9.1870420e-02 2.9480872e-01 1.8597995e-04]\n",
      "predicted Pose Class:  Running\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "predicted\n",
      "predictions:  [4.8906896e-01 2.0124689e-02 4.9057651e-01 2.2983288e-04]\n",
      "predicted Pose Class:  Walking\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "predicted\n",
      "predictions:  [7.6236367e-01 2.5897982e-02 2.1156481e-01 1.7356685e-04]\n",
      "predicted Pose Class:  Running\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "predicted\n",
      "predictions:  [5.5822849e-01 3.8010988e-03 4.3792608e-01 4.4338853e-05]\n",
      "predicted Pose Class:  Running\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "predicted\n",
      "predictions:  [0.9677963  0.00789314 0.00741785 0.01689263]\n",
      "predicted Pose Class:  Running\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "predicted\n",
      "predictions:  [0.22099671 0.6931014  0.0306822  0.05521961]\n",
      "predicted Pose Class:  Standing\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "predicted\n",
      "predictions:  [0.06661759 0.66067034 0.26302257 0.00968954]\n",
      "predicted Pose Class:  Standing\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "predicted\n",
      "predictions:  [0.56262213 0.23723106 0.0202891  0.17985776]\n",
      "predicted Pose Class:  Running\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "predicted\n",
      "predictions:  [7.9294279e-02 3.7448329e-03 2.3835976e-04 9.1672242e-01]\n",
      "predicted Pose Class:  Waving\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "predicted\n",
      "predictions:  [2.9839944e-02 3.2789934e-02 2.3667490e-04 9.3713349e-01]\n",
      "predicted Pose Class:  Waving\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "predicted\n",
      "predictions:  [2.6294837e-02 9.2049491e-01 5.2778989e-02 4.3126702e-04]\n",
      "predicted Pose Class:  Standing\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "predicted\n",
      "predictions:  [0.13346045 0.3167504  0.50259656 0.04719263]\n",
      "predicted Pose Class:  Walking\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n"
     ]
    }
   ],
   "source": [
    "# Set Window normal so we can resize it\n",
    "cv2.namedWindow('frame', cv2.WINDOW_NORMAL)\n",
    "\n",
    "# This is a test video\n",
    "cap = cv2.VideoCapture('http://192.168.43.1:8080/video')\n",
    "\n",
    "# Read the video stream from the camera\n",
    "# cap = cv2.VideoCapture('http://192.168.43.1:8080/video')\n",
    "\n",
    "# Get width and height of the frame\n",
    "width = int(cap.get(3))\n",
    "height = int(cap.get(4))\n",
    "\n",
    "# Read and store the credentials information in a dict\n",
    "with open('credential.txt', 'r') as myfile:\n",
    "  data = myfile.read()\n",
    "\n",
    "info_dict = eval(data)\n",
    "\n",
    "# Initialize the background Subtractor\n",
    "foog = cv2.createBackgroundSubtractorMOG2(detectShadows=True, varThreshold=100, history=2000)\n",
    "\n",
    "# Status is True when person is present and False when the person is not present.\n",
    "status = False\n",
    "\n",
    "# After the person disappears from view, wait at least 7 seconds before making the status False\n",
    "patience = 0.099\n",
    "\n",
    "# We don't consider an initial detection unless it's detected 15 times, this gets rid of false positives\n",
    "detection_thresh = 5\n",
    "\n",
    "# Initial time for calculating if patience time is up\n",
    "initial_time = None\n",
    "\n",
    "\n",
    "\n",
    "# We are creating a deque object of length detection_thresh and will store individual detection statuses here\n",
    "de = deque([False] * 10, maxlen=10)\n",
    "\n",
    "# Initialize an empty heap to store frames along with their negated percentage change\n",
    "frame_heap = []\n",
    "\n",
    "\n",
    "# Initialize these variables for calculating FPS\n",
    "fps = 0 \n",
    "frame_counter = 0\n",
    "start_time = time.time()\n",
    "counter = 0\n",
    "\n",
    "ret,prevframe = cap.read()\n",
    "initial_frame = prevframe\n",
    "recognisedName = \"\"\n",
    "sent  = False\n",
    "activity = []\n",
    "\n",
    "while True:\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break \n",
    "    \n",
    "            \n",
    "    # This function will return a boolean variable telling if someone was present or not, it will also draw boxes if it \n",
    "    # finds someone\n",
    "    detected, annotated_image = is_person_present(frame)  \n",
    "    # print(recognisedName)\n",
    "    # print('Hello')\n",
    "    \n",
    "\n",
    "    if detected and len(recognisedName) == 0: \n",
    "        # result = recogniseFromFace(frame)  \n",
    "        print('inside block')\n",
    "        result = recognize_faces(frame)  \n",
    "        # print(result)\n",
    "        activties = recognisize_pose(frame)\n",
    "        # print(activties)\n",
    "        activity.append(activties) \n",
    "        name , frame = result\n",
    "        print('inside block')\n",
    "        if name is not None: \n",
    "            if name: \n",
    "                recognisedName = name \n",
    "            else: \n",
    "                recognisedName = \"\"\n",
    "    # Register the current detection status on our deque object\n",
    "    de.appendleft(detected)\n",
    "    \n",
    "    # If we have consecutively detected a person 15 times then we are sure that someone is present    \n",
    "    # We also make this is the first time that this person has been detected so we only initialize the videowriter once\n",
    "    if sum(de) == detection_thresh and not status:                       \n",
    "            status = True\n",
    "            entry_time = datetime.datetime.now().strftime(\"%A, %I-%M-%S %p %d %B %Y\")\n",
    "            # out = cv2.VideoWriter('outputs/{}.mp4'.format(entry_time), cv2.VideoWriter_fourcc(*'XVID'), 15.0, (width, height))\n",
    "\n",
    "    # If status is True but the person is not in the current frame\n",
    "    if status and not detected:\n",
    "        \n",
    "        # Restart the patience timer only if the person has not been detected for a few frames so we are sure it wasn't a \n",
    "        # False positive\n",
    "        print('i reached deepest of the darkest point')\n",
    "        if sum(de) > (detection_thresh/2): \n",
    "            if initial_time is None:\n",
    "                print('initial time as been setted to',initial_time)\n",
    "                initial_time = time.time()\n",
    "                \n",
    "            \n",
    "            elif initial_time is not None:        \n",
    "                print('now the initial time is not null beacuse current time is ',initial_time)\n",
    "                print(time.time() - initial_time)\n",
    "                # If the patience has run out and the person is still not detected then set the status to False\n",
    "                # Also save the video by releasing the video writer and send a text message.\n",
    "                if  time.time() - initial_time >= patience:\n",
    "                    status = False\n",
    "                    print(\"inside the loop\")\n",
    "                    exit_time = datetime.datetime.now().strftime(\"%A, %I:%M:%S %p %d %B %Y\")\n",
    "                    # out.release()\n",
    "                    # print(out)\n",
    "                    initial_time = None\n",
    "                    de = deque([False] * len(de))\n",
    "                    element_counts = Counter(activity)\n",
    "                    \n",
    "                    # Find the most common element\n",
    "                    most_common_element = element_counts.most_common(1)[0][0]\n",
    "                    body = \"Alert: \\nA Person Entered the Room at {} \\nLeft the room at {} and did \".format(entry_time, exit_time,most_common_element) \n",
    "                    # Use Counter to count occurrences of each element\n",
    "                    activity.clear()\n",
    "                    if len(recognisedName) > 0: \n",
    "                        body = \"Alert: {} Has entered in your Room at {} \\n Left the room at {} and did {}\".format(recognisedName,entry_time,exit_time, most_common_element)\n",
    "                    print('message has been sent')\n",
    "                    send_message(body, info_dict)\n",
    "                    frame_index = 0\n",
    "                    cv2.destroyWindow(\"frame\")\n",
    "                    while frame_heap:\n",
    "                        # Pop the frame with the maximum percentage change (negated)\n",
    "                        neg_change_percentage, frame , date = heapq.heappop(frame_heap)\n",
    "                        change_percentage = -neg_change_percentage\n",
    "                        file_path = 'DetectedPerson'\n",
    "                        \n",
    "                        # Save the frame to disk\n",
    "                        cv2.imwrite(f'{file_path}/{date}.jpg', frame)\n",
    "                        frame_index += 1\n",
    "                    print(\"saved frame succesfully\")\n",
    "                    continue\n",
    "                    \n",
    "                \n",
    "    \n",
    "    # If a significant amount of detections (more than half of detection_thresh) has occurred then we reset the Initial Time.\n",
    "    elif status and sum(de) > (detection_thresh/2):\n",
    "        change_percentage = calculate_percentage_change(prevframe, frame)\n",
    "        change_initial_change = calculate_percentage_change(initial_frame,frame)\n",
    "        # print(change_percentage)\n",
    "        if change_percentage > 95 and sent == False: \n",
    "            sent = True\n",
    "            body = \"Alert: \\n Some one has changed the camera configuration at:{}\".format(datetime.datetime.now().strftime(\"%A, %I-%M-%S %p %d %B %Y\")) \n",
    "            if recognisedName != 'Unknown': \n",
    "                body = \"Alert: \\n {} has changed the camera configuration at:{}\".format(recognisedName, datetime.datetime.now().strftime(\"%A, %I-%M-%S %p %d %B %Y\")) \n",
    "            send_message(body,info_dict)\n",
    "    \n",
    "        if change_initial_change > 60: \n",
    "            heapq.heappush(frame_heap, (-change_percentage, frame,datetime.datetime.now().strftime(\"%A, %I-%M-%S %p %d %B %Y\")))\n",
    "        prevframe = frame\n",
    "        initial_time = None\n",
    "    \n",
    "    # Get the current time in the required format\n",
    "    current_time = datetime.datetime.now().strftime(\"%A, %I:%M:%S %p %d %B %Y\")\n",
    "\n",
    "    # Display the FPS\n",
    "    cv2.putText(annotated_image, 'FPS: {:.2f}'.format(fps), (510, 450), cv2.FONT_HERSHEY_COMPLEX, 0.6, (255, 40, 155), 2)\n",
    "    \n",
    "    # Display Time\n",
    "    cv2.putText(annotated_image, current_time, (310, 20), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 255), 1)    \n",
    "    \n",
    "    # Display the Room Status\n",
    "    cv2.putText(annotated_image, 'Room Occupied: {}'.format(str(status)), (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, \n",
    "                (200, 10, 150), 2)\n",
    "\n",
    "    # Show the patience Value\n",
    "    if initial_time is None:\n",
    "        text = 'Patience: {}'.format(patience)\n",
    "    else: \n",
    "        text = 'Patience: {:.2f}'.format(max(0, patience - (time.time() - initial_time)))\n",
    "        \n",
    "    cv2.putText(annotated_image, text, (10, 450), cv2.FONT_HERSHEY_COMPLEX, 0.6, (255, 40, 155), 2)   \n",
    "\n",
    " \n",
    "    # Show the Frame\n",
    "    cv2.imshow('frame', frame)\n",
    "    \n",
    "    # Calculate the Average FPS\n",
    "    frame_counter += 1\n",
    "    fps = (frame_counter / (time.time() - start_time))\n",
    "    \n",
    "    \n",
    "    # Exit if q is pressed.\n",
    "    if cv2.waitKey(30) == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "# Release Capture and destroy windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34bbbcc-9641-4f67-be10-5a2ab5323196",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806d4e6b-6092-4552-a1ec-40399a355fd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
