{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f617e3c7-3285-4bb7-9d70-675652e4fd9d",
   "metadata": {},
   "source": [
    "# First Getting the face recognition Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7417211e-5911-4372-aac3-2e683cce4e91",
   "metadata": {},
   "source": [
    "### A. Loading the Model of face recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df29e38c-0def-40b5-b719-086d60e47377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Pushkar', 1: 'Sairaj', 2: 'Tushar'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n",
      "confidence 0.34346375\n",
      "[[0.315617   0.34346375 0.34091926]]\n",
      "0.34346375\n",
      "29.471456248876287\n",
      "Sairaj\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "confidence 0.35209048\n",
      "[[0.32219616 0.35209048 0.32571328]]\n",
      "0.35209048\n",
      "37.2602401099915\n",
      "Sairaj\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "confidence 0.3565634\n",
      "[[0.31499267 0.3565634  0.32844394]]\n",
      "0.3565634\n",
      "86.57282547041586\n",
      "Sairaj\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "confidence 0.3601562\n",
      "[[0.31297693 0.3601562  0.32686684]]\n",
      "0.3601562\n",
      "84.98550713189086\n",
      "Sairaj\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "confidence 0.35973904\n",
      "[[0.31213808 0.35973904 0.32812288]]\n",
      "0.35973904\n",
      "82.6487274019187\n",
      "Sairaj\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "confidence 0.3605732\n",
      "[[0.3121891 0.3605732 0.3272377]]\n",
      "0.3605732\n",
      "73.98890580909456\n",
      "Sairaj\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "confidence 0.36456034\n",
      "[[0.31369796 0.36456034 0.32174167]]\n",
      "0.36456034\n",
      "97.29312473732182\n",
      "Sairaj\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "confidence 0.36529616\n",
      "[[0.31283712 0.36529616 0.3218668 ]]\n",
      "0.36529616\n",
      "86.23278872422759\n",
      "Sairaj\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "confidence 0.36509845\n",
      "[[0.31316477 0.36509845 0.32173678]]\n",
      "0.36509845\n",
      "100.49684763850841\n",
      "Sairaj\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "confidence 0.3665928\n",
      "[[0.312926   0.3665928  0.32048124]]\n",
      "0.3665928\n",
      "98.75629369064028\n",
      "Sairaj\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "confidence 0.3657932\n",
      "[[0.3144941  0.3657932  0.31971267]]\n",
      "0.3657932\n",
      "101.18793308286274\n",
      "Sairaj\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "confidence 0.36456758\n",
      "[[0.3120373  0.36456758 0.3233952 ]]\n",
      "0.36456758\n",
      "109.16585586663575\n",
      "Sairaj\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "confidence 0.36620128\n",
      "[[0.31234834 0.36620128 0.32145032]]\n",
      "0.36620128\n",
      "91.95430872148\n",
      "Sairaj\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "confidence 0.3625095\n",
      "[[0.31282672 0.3625095  0.32466376]]\n",
      "0.3625095\n",
      "91.75619012823283\n",
      "Sairaj\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "confidence 0.359993\n",
      "[[0.3110098  0.359993   0.32899722]]\n",
      "0.359993\n",
      "107.56239558594285\n",
      "Sairaj\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "confidence 0.36156484\n",
      "[[0.3127201  0.36156484 0.3257151 ]]\n",
      "0.36156484\n",
      "90.94623397751883\n",
      "Sairaj\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "confidence 0.3592807\n",
      "[[0.31240356 0.3592807  0.3283158 ]]\n",
      "0.3592807\n",
      "104.37648076532058\n",
      "Sairaj\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "confidence 0.36055797\n",
      "[[0.3128343  0.36055797 0.32660773]]\n",
      "0.36055797\n",
      "93.14392707682221\n",
      "Sairaj\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "confidence 0.3589831\n",
      "[[0.3126352 0.3589831 0.3283817]]\n",
      "0.3589831\n",
      "101.79584087427276\n",
      "Sairaj\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "confidence 0.35710326\n",
      "[[0.31362844 0.35710326 0.32926828]]\n",
      "0.35710326\n",
      "97.74558413962421\n",
      "Sairaj\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "confidence 0.35992223\n",
      "[[0.31190187 0.35992223 0.32817593]]\n",
      "0.35992223\n",
      "99.53016281041339\n",
      "Sairaj\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "confidence 0.35959685\n",
      "[[0.3128832  0.35959685 0.32751995]]\n",
      "0.35959685\n",
      "100.25827174779336\n",
      "Sairaj\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "confidence 0.3636713\n",
      "[[0.31210405 0.3636713  0.32422462]]\n",
      "0.3636713\n",
      "90.04835864464177\n",
      "Sairaj\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "confidence 0.36439124\n",
      "[[0.31407636 0.36439124 0.32153234]]\n",
      "0.36439124\n",
      "92.69878556486265\n",
      "Sairaj\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "confidence 0.36370668\n",
      "[[0.31718528 0.36370668 0.319108  ]]\n",
      "0.36370668\n",
      "95.82960530350528\n",
      "Sairaj\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "confidence 0.3613218\n",
      "[[0.32045528 0.3613218  0.3182229 ]]\n",
      "0.3613218\n",
      "95.63120615320602\n",
      "Sairaj\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "confidence 0.35814115\n",
      "[[0.32357532 0.35814115 0.3182835 ]]\n",
      "0.35814115\n",
      "95.84274838708889\n",
      "Sairaj\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "confidence 0.35921222\n",
      "[[0.31935686 0.35921222 0.321431  ]]\n",
      "0.35921222\n",
      "96.41284962029074\n",
      "Sairaj\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "confidence 0.3581535\n",
      "[[0.31803995 0.3581535  0.32380652]]\n",
      "0.3581535\n",
      "96.12739822815072\n",
      "Sairaj\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "confidence 0.35658652\n",
      "[[0.31783473 0.35658652 0.32557878]]\n",
      "0.35658652\n",
      "93.55557646312053\n",
      "Sairaj\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "confidence 0.35658824\n",
      "[[0.3173205  0.35658824 0.3260913 ]]\n",
      "0.35658824\n",
      "95.35011183885624\n",
      "Sairaj\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "confidence 0.35516253\n",
      "[[0.32039037 0.35516253 0.3244471 ]]\n",
      "0.35516253\n",
      "88.77847383693585\n",
      "Sairaj\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "confidence 0.3549695\n",
      "[[0.32035667 0.3549695  0.3246738 ]]\n",
      "0.3549695\n",
      "98.69021298686175\n",
      "Sairaj\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "confidence 0.35268962\n",
      "[[0.32219893 0.35268962 0.32511145]]\n",
      "0.35268962\n",
      "100.23029153514877\n",
      "Sairaj\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "confidence 0.3527983\n",
      "[[0.3218457  0.3527983  0.32535604]]\n",
      "0.3527983\n",
      "95.96120879362883\n",
      "Sairaj\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "confidence 0.35152304\n",
      "[[0.32205954 0.35152304 0.32641742]]\n",
      "0.35152304\n",
      "95.30472018130195\n",
      "Sairaj\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "confidence 0.35083213\n",
      "[[0.32274845 0.35083213 0.3264195 ]]\n",
      "0.35083213\n",
      "85.41945749602255\n",
      "Sairaj\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "confidence 0.34965417\n",
      "[[0.32271656 0.34965417 0.32762927]]\n",
      "0.34965417\n",
      "97.05798936358723\n",
      "Sairaj\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "confidence 0.3500275\n",
      "[[0.32208455 0.3500275  0.3278879 ]]\n",
      "0.3500275\n",
      "95.20364198397921\n",
      "Sairaj\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "confidence 0.34898666\n",
      "[[0.3224934  0.34898666 0.32851994]]\n",
      "0.34898666\n",
      "87.77002375116446\n",
      "Sairaj\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "import keras\n",
    "\n",
    "mp = keras.models.load_model(\"Models/FaceRecognitionModel.keras\")\n",
    "recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "recognizer.read(\"Models/ModelToRecogniseUnrecogniseFace.yml\")\n",
    "\n",
    "# Load the face classifier\n",
    "face_classifier = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "# Load the face map for label mapping\n",
    "with open(\"ResultsMap.pkl\", 'rb') as fileReadStream:\n",
    "    ResultMap = pickle.load(fileReadStream)\n",
    "\n",
    "print(ResultMap)\n",
    "# Initialize the webcam\n",
    "webcam = cv2.VideoCapture(\"sample_video1.mp4\")\n",
    "\n",
    "# Function to recognize faces\n",
    "def recognize_faces(frame):\n",
    "    # Convert the frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in the frame\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "    name = ''\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Extract the face ROI\n",
    "        face_roi = frame[y:y+h, x:x+w]\n",
    "        face_roi1 = gray[y:y+h, x:x+w]\n",
    "        \n",
    "        label_id, confidence = recognizer.predict(face_roi1)\n",
    "\n",
    "        # Resize the face ROI to match the input size of the model\n",
    "        resized_face = cv2.resize(face_roi, (64, 64))\n",
    "\n",
    "        # Preprocess the face (normalize pixel values)\n",
    "        normalized_face = resized_face / 255.0\n",
    "\n",
    "        # Reshape the face for the model input (add batch dimension)\n",
    "        reshaped_face = np.expand_dims(normalized_face, axis=0)\n",
    "\n",
    "        # Perform prediction using the model\n",
    "        prediction = mp.predict(reshaped_face)\n",
    "        predicted_label = ResultMap[np.argmax(prediction)]\n",
    "        print('confidence',prediction[0][np.argmax(prediction)])\n",
    "        print(prediction)\n",
    "        # print(predicted_label)\n",
    "        print(np.max(prediction))\n",
    "        name = predicted_label\n",
    "        pre = np.max(prediction)\n",
    "        print(confidence)\n",
    "        if confidence > 110.07375104700013: \n",
    "            cv2.putText(frame, 'Unknown', (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2) \n",
    "            name = 'Unknown'\n",
    "        else:\n",
    "            cv2.putText(frame, predicted_label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n",
    "\n",
    "\n",
    "        # Draw the bounding box and label on the frame\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        \n",
    "\n",
    "    return frame , name\n",
    "\n",
    "# Main loop to capture frames from the webcam\n",
    "while True:\n",
    "    # Read a frame from the webcam\n",
    "    ret, frame = webcam.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Perform face recognition on the frame\n",
    "    frame , name = recognize_faces(frame)\n",
    "    print(name)\n",
    "    # Display the frame\n",
    "    cv2.imshow('Face Recognition', frame)\n",
    "\n",
    "    # Check for 'q' key to quit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close all OpenCV windows\n",
    "webcam.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bffa79-4b8b-480c-85ee-e6494d46333f",
   "metadata": {},
   "source": [
    "## B. Loading the model of the action recongition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a52ecc8-5b61-45e3-9017-e84da213d532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n",
      "predictions:  [4.1327358e-04 9.9323118e-01 6.1787656e-03 1.7673564e-04]\n",
      "predicted Pose Class:  Standing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "predictions:  [5.9373397e-04 9.9327725e-01 6.0086939e-03 1.2034838e-04]\n",
      "predicted Pose Class:  Standing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "predictions:  [7.9373072e-04 9.9275851e-01 6.3235676e-03 1.2420100e-04]\n",
      "predicted Pose Class:  Standing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "predictions:  [6.0241087e-04 9.9399370e-01 5.2793478e-03 1.2459271e-04]\n",
      "predicted Pose Class:  Standing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "predictions:  [7.3159731e-04 9.9471986e-01 4.4303038e-03 1.1816680e-04]\n",
      "predicted Pose Class:  Standing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "predictions:  [1.3630603e-03 9.9259514e-01 5.9079872e-03 1.3388120e-04]\n",
      "predicted Pose Class:  Standing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "predictions:  [1.8013919e-03 9.9166846e-01 6.4050741e-03 1.2502850e-04]\n",
      "predicted Pose Class:  Standing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "predictions:  [1.8900551e-03 9.9191177e-01 6.0756193e-03 1.2247435e-04]\n",
      "predicted Pose Class:  Standing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "predictions:  [2.0043438e-03 9.9163753e-01 6.2278179e-03 1.3030750e-04]\n",
      "predicted Pose Class:  Standing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "predictions:  [1.1097969e-03 9.9225557e-01 6.4940718e-03 1.4053237e-04]\n",
      "predicted Pose Class:  Standing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "predictions:  [1.6013099e-03 9.8990166e-01 8.3366139e-03 1.6036781e-04]\n",
      "predicted Pose Class:  Standing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "predictions:  [1.6897746e-03 9.8869097e-01 9.4441231e-03 1.7510189e-04]\n",
      "predicted Pose Class:  Standing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "predictions:  [1.4801731e-03 9.8899138e-01 9.3608852e-03 1.6747481e-04]\n",
      "predicted Pose Class:  Standing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "predictions:  [1.5241867e-03 9.8813522e-01 1.0176278e-02 1.6436324e-04]\n",
      "predicted Pose Class:  Standing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "predictions:  [8.7343907e-04 9.9065745e-01 8.3093671e-03 1.5973457e-04]\n",
      "predicted Pose Class:  Standing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "predictions:  [8.8985864e-04 9.8836333e-01 1.0549567e-02 1.9720463e-04]\n",
      "predicted Pose Class:  Standing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "predictions:  [5.4138753e-04 9.9100459e-01 8.2495585e-03 2.0447811e-04]\n",
      "predicted Pose Class:  Standing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "predictions:  [4.9155188e-04 9.9015605e-01 9.1422712e-03 2.1012999e-04]\n",
      "predicted Pose Class:  Standing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "predictions:  [7.7331997e-04 9.8886651e-01 1.0171637e-02 1.8845966e-04]\n",
      "predicted Pose Class:  Standing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "predictions:  [9.0661622e-04 9.9028790e-01 8.6305924e-03 1.7481811e-04]\n",
      "predicted Pose Class:  Standing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "predictions:  [1.5141849e-03 9.8780209e-01 1.0475688e-02 2.0811534e-04]\n",
      "predicted Pose Class:  Standing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "predictions:  [1.7424223e-03 9.8976642e-01 8.3387746e-03 1.5229649e-04]\n",
      "predicted Pose Class:  Standing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "predictions:  [1.3581017e-03 9.9090356e-01 7.5818538e-03 1.5649294e-04]\n",
      "predicted Pose Class:  Standing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "predictions:  [1.2478228e-03 9.9117392e-01 7.4239066e-03 1.5445644e-04]\n",
      "predicted Pose Class:  Standing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "predictions:  [1.6280626e-03 9.9050421e-01 7.7128881e-03 1.5479005e-04]\n",
      "predicted Pose Class:  Standing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "predictions:  [1.3462111e-03 9.9006420e-01 8.4061036e-03 1.8348025e-04]\n",
      "predicted Pose Class:  Standing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "predictions:  [1.6872686e-03 9.9222821e-01 5.9554558e-03 1.2907581e-04]\n",
      "predicted Pose Class:  Standing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "predictions:  [6.9555419e-04 9.9398273e-01 5.1914924e-03 1.3028004e-04]\n",
      "predicted Pose Class:  Standing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "predictions:  [1.7681047e-03 9.9144858e-01 6.6471007e-03 1.3610812e-04]\n",
      "predicted Pose Class:  Standing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "predictions:  [1.7057346e-03 9.9138266e-01 6.7780525e-03 1.3353454e-04]\n",
      "predicted Pose Class:  Standing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "predictions:  [1.9698150e-03 9.9030161e-01 7.5852661e-03 1.4330607e-04]\n",
      "predicted Pose Class:  Standing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "predictions:  [1.9701382e-03 9.8972565e-01 8.1588570e-03 1.4539146e-04]\n",
      "predicted Pose Class:  Standing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "predictions:  [1.6701774e-03 9.9128073e-01 6.9283568e-03 1.2082784e-04]\n",
      "predicted Pose Class:  Standing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "predictions:  [1.93650543e-03 9.91613269e-01 6.33483753e-03 1.15374554e-04]\n",
      "predicted Pose Class:  Standing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "predictions:  [1.28304271e-03 9.91724849e-01 6.87084906e-03 1.21220684e-04]\n",
      "predicted Pose Class:  Standing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "predictions:  [1.1801347e-03 9.9220830e-01 6.4893146e-03 1.2224556e-04]\n",
      "predicted Pose Class:  Standing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "predictions:  [6.8634358e-04 9.9281269e-01 6.3634906e-03 1.3750856e-04]\n",
      "predicted Pose Class:  Standing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "predictions:  [9.074143e-04 9.922950e-01 6.637325e-03 1.602599e-04]\n",
      "predicted Pose Class:  Standing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "predictions:  [6.8599655e-04 9.9326646e-01 5.8928533e-03 1.5466346e-04]\n",
      "predicted Pose Class:  Standing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "predictions:  [7.1120582e-04 9.9377924e-01 5.3612334e-03 1.4826978e-04]\n",
      "predicted Pose Class:  Standing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "predictions:  [7.0369209e-04 9.9345857e-01 5.6856954e-03 1.5205982e-04]\n",
      "predicted Pose Class:  Standing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "predictions:  [1.6581162e-03 9.9381590e-01 4.4153561e-03 1.1066207e-04]\n",
      "predicted Pose Class:  Standing\n",
      "[INFO] Out video Saved as 'output.avi'\n",
      "[INFO] Inference on Videostream is Ended...\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers, Sequential\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "\n",
    "import mediapipe as mp\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os \n",
    "\n",
    "\n",
    "# ap = argparse.ArgumentParser()\n",
    "# ap.add_argument(\"-m\", \"--model\", type=str, required=True,\n",
    "#                 help=\"path to saved .h5 model, eg: dir/model.h5\")\n",
    "# ap.add_argument(\"-c\", \"--conf\", type=float, required=True,\n",
    "#                 help=\"min prediction conf to detect pose class (0<conf<1)\")\n",
    "# ap.add_argument(\"-i\", \"--source\", type=str, required=True,\n",
    "#                 help=\"path to sample image\")\n",
    "# ap.add_argument(\"--save\", action='store_true',\n",
    "#                 help=\"Save video\")\n",
    "\n",
    "# args = vars(ap.parse_args())\n",
    "source = \"Video_5.mp4\"\n",
    "path_saved_model = \"Models/Action Recognition/model.keras\"\n",
    "threshold = 0.2\n",
    "save = \"Output\"\n",
    "print('Hello')\n",
    "##############\n",
    "torso_size_multiplier = 2.5\n",
    "n_landmarks = 33\n",
    "n_dimensions = 3\n",
    "landmark_names = [\n",
    "    'nose',\n",
    "    'left_eye_inner', 'left_eye', 'left_eye_outer',\n",
    "    'right_eye_inner', 'right_eye', 'right_eye_outer',\n",
    "    'left_ear', 'right_ear',\n",
    "    'mouth_left', 'mouth_right',\n",
    "    'left_shoulder', 'right_shoulder',\n",
    "    'left_elbow', 'right_elbow',\n",
    "    'left_wrist', 'right_wrist',\n",
    "    'left_pinky_1', 'right_pinky_1',\n",
    "    'left_index_1', 'right_index_1',\n",
    "    'left_thumb_2', 'right_thumb_2',\n",
    "    'left_hip', 'right_hip',\n",
    "    'left_knee', 'right_knee',\n",
    "    'left_ankle', 'right_ankle',\n",
    "    'left_heel', 'right_heel',\n",
    "    'left_foot_index', 'right_foot_index',\n",
    "]\n",
    "class_names = [\n",
    "    'Running','Standing','Walking','Waving'\n",
    "]\n",
    "##############\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "col_names = []\n",
    "for i in range(n_landmarks):\n",
    "    name = mp_pose.PoseLandmark(i).name\n",
    "    name_x = name + '_X'\n",
    "    name_y = name + '_Y'\n",
    "    name_z = name + '_Z'\n",
    "    name_v = name + '_V'\n",
    "    col_names.append(name_x)\n",
    "    col_names.append(name_y)\n",
    "    col_names.append(name_z)\n",
    "    col_names.append(name_v)\n",
    "\n",
    "# Load saved model\n",
    "model = load_model(path_saved_model, compile=True)\n",
    "\n",
    "if source.endswith(('.jpg', '.jpeg', '.png')):\n",
    "    path_to_img = source\n",
    "    print('Entered1')\n",
    "    # Load sample Image\n",
    "    \n",
    "    img = cv2.imread(path_to_img)\n",
    "    print(img)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    result = pose.process(img_rgb)\n",
    "    if result.pose_landmarks:\n",
    "        lm_list = []\n",
    "        for landmarks in result.pose_landmarks.landmark:\n",
    "            # Preprocessing\n",
    "            max_distance = 0\n",
    "            lm_list.append(landmarks)\n",
    "        print('landmarks')\n",
    "        print(landmarks)\n",
    "        center_x = (lm_list[landmark_names.index('right_hip')].x +\n",
    "                    lm_list[landmark_names.index('left_hip')].x)*0.5\n",
    "        center_y = (lm_list[landmark_names.index('right_hip')].y +\n",
    "                    lm_list[landmark_names.index('left_hip')].y)*0.5\n",
    "\n",
    "        shoulders_x = (lm_list[landmark_names.index('right_shoulder')].x +\n",
    "                       lm_list[landmark_names.index('left_shoulder')].x)*0.5\n",
    "        shoulders_y = (lm_list[landmark_names.index('right_shoulder')].y +\n",
    "                       lm_list[landmark_names.index('left_shoulder')].y)*0.5\n",
    "\n",
    "        for lm in lm_list:\n",
    "            distance = math.sqrt((lm.x - center_x)**2 + (lm.y - center_y)**2)\n",
    "            if(distance > max_distance):\n",
    "                max_distance = distance\n",
    "        torso_size = math.sqrt((shoulders_x - center_x) **\n",
    "                               2 + (shoulders_y - center_y)**2)\n",
    "        max_distance = max(torso_size*torso_size_multiplier, max_distance)\n",
    "\n",
    "        pre_lm = list(np.array([[(landmark.x-center_x)/max_distance, (landmark.y-center_y)/max_distance,\n",
    "                                 landmark.z/max_distance, landmark.visibility] for landmark in lm_list]).flatten())\n",
    "        data = pd.DataFrame([pre_lm], columns=col_names)\n",
    "        print(data)\n",
    "        predict = model.predict(data)[0]\n",
    "        print('predicted')\n",
    "        if max(predict) > threshold:\n",
    "            pose_class = class_names[predict.argmax()]\n",
    "            print('predictions: ', predict)\n",
    "            print('predicted Pose Class: ', pose_class)\n",
    "        else:\n",
    "            pose_class = 'Unknown Pose'\n",
    "            print('[INFO] Predictions is below given Confidence!!')\n",
    "\n",
    "    # Show Result\n",
    "    img = cv2.putText(\n",
    "        img, f'{class_names[predict.argmax()]}',\n",
    "        (40, 50), cv2.FONT_HERSHEY_PLAIN,\n",
    "        2, (255, 0, 255), 2\n",
    "    )\n",
    "\n",
    "    if save:\n",
    "        os.makedirs('ImageOutput', exist_ok=True)\n",
    "        img_full_name = os.path.split(path_to_img)[1]\n",
    "        img_name = os.path.splitext(img_full_name)[0]\n",
    "        path_to_save_img = f'ImageOutput/{img_name}.jpg'\n",
    "        cv2.imwrite(f'{path_to_save_img}', img)\n",
    "        print(f'[INFO] Output Image Saved in {path_to_save_img}')\n",
    "\n",
    "    cv2.imshow('Output Image', img)\n",
    "    if cv2.waitKey(0) & 0xFF == ord('q'):\n",
    "        cv2.destroyAllWindows()\n",
    "    print('[INFO] Inference on Test Image is Ended...')\n",
    "\n",
    "else:\n",
    "    # Web-cam\n",
    "    if source.isnumeric():\n",
    "        source = int(source)\n",
    "\n",
    "    cap = cv2.VideoCapture(source)\n",
    "    source_width = int(cap.get(3))\n",
    "    source_height = int(cap.get(4))\n",
    "\n",
    "    # Write Video\n",
    "    if save:\n",
    "        out_video = cv2.VideoWriter('output.avi', \n",
    "                            cv2.VideoWriter_fourcc(*'MJPG'),\n",
    "                            10, (source_width, source_height))\n",
    "\n",
    "    while True:\n",
    "        success, img = cap.read()\n",
    "        if not success:\n",
    "            print('[ERROR] Failed to Read Video feed')\n",
    "            break\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        result = pose.process(img_rgb)\n",
    "\n",
    "        if result.pose_landmarks:\n",
    "            lm_list = []\n",
    "            for landmarks in result.pose_landmarks.landmark:\n",
    "                # Preprocessing\n",
    "                max_distance = 0\n",
    "                lm_list.append(landmarks)\n",
    "            center_x = (lm_list[landmark_names.index('right_hip')].x +\n",
    "                        lm_list[landmark_names.index('left_hip')].x)*0.5\n",
    "            center_y = (lm_list[landmark_names.index('right_hip')].y +\n",
    "                        lm_list[landmark_names.index('left_hip')].y)*0.5\n",
    "\n",
    "            shoulders_x = (lm_list[landmark_names.index('right_shoulder')].x +\n",
    "                           lm_list[landmark_names.index('left_shoulder')].x)*0.5\n",
    "\n",
    "            \n",
    "            shoulders_y = (lm_list[landmark_names.index('right_shoulder')].y +\n",
    "                           lm_list[landmark_names.index('left_shoulder')].y)*0.5\n",
    "\n",
    "            for lm in lm_list:\n",
    "                distance = math.sqrt((lm.x - center_x) **\n",
    "                                     2 + (lm.y - center_y)**2)\n",
    "                if(distance > max_distance):\n",
    "                    max_distance = distance\n",
    "            torso_size = math.sqrt((shoulders_x - center_x) **\n",
    "                                   2 + (shoulders_y - center_y)**2)\n",
    "            max_distance = max(torso_size*torso_size_multiplier, max_distance)\n",
    "\n",
    "            pre_lm = list(np.array([[(landmark.x-center_x)/max_distance, (landmark.y-center_y)/max_distance,\n",
    "                                     landmark.z/max_distance, landmark.visibility] for landmark in lm_list]).flatten())\n",
    "            data = pd.DataFrame([pre_lm], columns=col_names)\n",
    "            # print(data[2])\n",
    "            predict = model.predict(data)[0]\n",
    "            if max(predict) > threshold:\n",
    "                pose_class = class_names[predict.argmax()]\n",
    "                print('predictions: ', predict)\n",
    "                print('predicted Pose Class: ', pose_class)\n",
    "            else:\n",
    "                pose_class = 'Unknown Pose'\n",
    "                print('[INFO] Predictions is below given Confidence!!')\n",
    "\n",
    "            # Show Result\n",
    "            img = cv2.putText(\n",
    "                img, f'{pose_class}',\n",
    "                (40, 50), cv2.FONT_HERSHEY_PLAIN,\n",
    "                2, (255, 0, 255), 2\n",
    "            )\n",
    "        # Write Video\n",
    "        if save:\n",
    "            out_video.write(img)\n",
    "\n",
    "        cv2.imshow('Output Image', img)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    if save:\n",
    "        out_video.release()\n",
    "        print(\"[INFO] Out video Saved as 'output.avi'\")\n",
    "    cv2.destroyAllWindows()\n",
    "    print('[INFO] Inference on Videostream is Ended...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9105beb8-c397-4bdf-a91b-e072ed3d44b1",
   "metadata": {},
   "source": [
    "## C. Intrusion Detection code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d213ee1-2d54-4701-9ad0-f4c410fd0b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import Counter\n",
    "import keras\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import datetime\n",
    "from collections import deque\n",
    "from twilio.rest import Client \n",
    "import random\n",
    "import heapq\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import requests\n",
    "import keras\n",
    "from keras import layers, Sequential\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "import mediapipe as mp\n",
    "import glob\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22345f33-69d1-463c-b2ef-f95f541c6160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no error in functions\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def is_person_present(frame, thresh=1100):\n",
    "    kernel = None\n",
    "    global foog\n",
    "    \n",
    "    # Apply background subtraction\n",
    "    fgmask = foog.apply(frame)\n",
    "\n",
    "    # Get rid of the shadows\n",
    "    ret, fgmask = cv2.threshold(fgmask, 250, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Apply some morphological operations to make sure you have a good mask\n",
    "    fgmask = cv2.dilate(fgmask, kernel, iterations=4)\n",
    "\n",
    "    # Detect contours in the frame\n",
    "    contours, hierarchy = cv2.findContours(fgmask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "     \n",
    "    # Check if there was a contour and the area is somewhat higher than some threshold so we know it's a person and not noise\n",
    "    if contours and cv2.contourArea(max(contours, key=cv2.contourArea)) > thresh:\n",
    "            \n",
    "        # Get the max contour\n",
    "        cnt = max(contours, key=cv2.contourArea)\n",
    "\n",
    "        # Draw a bounding box around the person and label it as person detected\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
    "        cv2.putText(frame, 'Person Detected', (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "        \n",
    "        return True, frame\n",
    "        \n",
    "        \n",
    "    # Otherwise report there was no one present\n",
    "    else:\n",
    "        return False, frame\n",
    "\n",
    "\n",
    "def send_message(body, info_dict):\n",
    "\n",
    "    # Your Account SID from twilio.com/console\n",
    "    account_sid = info_dict['account_sid']\n",
    "\n",
    "    # Your Auth Token from twilio.com/console\n",
    "    auth_token  = info_dict['auth_token']\n",
    "\n",
    "\n",
    "    client = Client(account_sid, auth_token)\n",
    "\n",
    "    message = client.messages.create(to=info_dict['your_num'], from_=info_dict['trial_num'], body=body)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_percentage_change(frame1, frame2):\n",
    "    # Convert frames to grayscale\n",
    "    gray_frame1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "    gray_frame2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Calculate absolute difference between the two frames\n",
    "    abs_diff = cv2.absdiff(gray_frame1, gray_frame2)\n",
    "\n",
    "    # Calculate percentage change\n",
    "    percentage_change = (np.count_nonzero(abs_diff) / abs_diff.size) * 100\n",
    "\n",
    "    return percentage_change\n",
    "\n",
    "\n",
    "def save_frame(frame_heap): \n",
    "    frame_index = 0\n",
    "    while frame_heap:\n",
    "        # Pop the frame with the maximum percentage change (negated)\n",
    "        neg_change_percentage, frame , date = heapq.heappop(frame_heap)\n",
    "        change_percentage = -neg_change_percentage\n",
    "        file_path = 'DetectedPerson'\n",
    "        # Save the frame to disk\n",
    "        cv2.imwrite(f'{file_path}/{date}.jpg', frame)\n",
    "        frame_index += 1\n",
    "    print(\"saved frame succesfully\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "print('no error in functions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51e8392-2f9e-4cbc-ab1b-56be340c2dad",
   "metadata": {},
   "source": [
    "## Final Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7261b3c7-6383-4e39-b6cd-9ffae621c74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i reached deepest of the darkest point\n",
      "initial time as been setted to None\n",
      "i reached deepest of the darkest point\n",
      "now the initial time is not null beacuse current time is  1713396499.097837\n",
      "0.2273693084716797\n",
      "inside the loop\n",
      "message has been sent\n",
      "saved frame succesfully\n"
     ]
    }
   ],
   "source": [
    "# Set Window normal so we can resize it\n",
    "cv2.namedWindow('frame', cv2.WINDOW_NORMAL)\n",
    "\n",
    "# This is a test video\n",
    "cap = cv2.VideoCapture('sample_video1.mp4')\n",
    "\n",
    "# Read the video stream from the camera\n",
    "# cap = cv2.VideoCapture('http://192.168.43.1:8080/video')\n",
    "\n",
    "# Get width and height of the frame\n",
    "width = int(cap.get(3))\n",
    "height = int(cap.get(4))\n",
    "\n",
    "# Read and store the credentials information in a dict\n",
    "with open('credential.txt', 'r') as myfile:\n",
    "  data = myfile.read()\n",
    "\n",
    "info_dict = eval(data)\n",
    "\n",
    "# Initialize the background Subtractor\n",
    "foog = cv2.createBackgroundSubtractorMOG2(detectShadows=True, varThreshold=100, history=2000)\n",
    "\n",
    "# Status is True when person is present and False when the person is not present.\n",
    "status = False\n",
    "\n",
    "# After the person disappears from view, wait at least 7 seconds before making the status False\n",
    "patience = 0.099\n",
    "\n",
    "# We don't consider an initial detection unless it's detected 15 times, this gets rid of false positives\n",
    "detection_thresh = 5\n",
    "\n",
    "# Initial time for calculating if patience time is up\n",
    "initial_time = None\n",
    "\n",
    "\n",
    "\n",
    "# We are creating a deque object of length detection_thresh and will store individual detection statuses here\n",
    "de = deque([False] * 10, maxlen=10)\n",
    "\n",
    "# Initialize an empty heap to store frames along with their negated percentage change\n",
    "frame_heap = []\n",
    "\n",
    "\n",
    "# Initialize these variables for calculating FPS\n",
    "fps = 0 \n",
    "frame_counter = 0\n",
    "start_time = time.time()\n",
    "counter = 0\n",
    "\n",
    "ret,prevframe = cap.read()\n",
    "initial_frame = prevframe\n",
    "recognisedName = \"\"\n",
    "sent  = False\n",
    "\n",
    "while True:\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break \n",
    "    \n",
    "            \n",
    "    # This function will return a boolean variable telling if someone was present or not, it will also draw boxes if it \n",
    "    # finds someone\n",
    "    detected, annotated_image = is_person_present(frame)  \n",
    "    # print(recognisedName)\n",
    "    if detected and len(recognisedName) == 0: \n",
    "        # result = recogniseFromFace(frame)  \n",
    "        result = ''\n",
    "        if result is not None: \n",
    "            result = ''\n",
    "    # Register the current detection status on our deque object\n",
    "    de.appendleft(detected)\n",
    "    \n",
    "    # If we have consecutively detected a person 15 times then we are sure that someone is present    \n",
    "    # We also make this is the first time that this person has been detected so we only initialize the videowriter once\n",
    "    if sum(de) == detection_thresh and not status:                       \n",
    "            status = True\n",
    "            entry_time = datetime.datetime.now().strftime(\"%A, %I-%M-%S %p %d %B %Y\")\n",
    "            # out = cv2.VideoWriter('outputs/{}.mp4'.format(entry_time), cv2.VideoWriter_fourcc(*'XVID'), 15.0, (width, height))\n",
    "\n",
    "    # If status is True but the person is not in the current frame\n",
    "    if status and not detected:\n",
    "        \n",
    "        # Restart the patience timer only if the person has not been detected for a few frames so we are sure it wasn't a \n",
    "        # False positive\n",
    "        print('i reached deepest of the darkest point')\n",
    "        if sum(de) > (detection_thresh/2): \n",
    "            if initial_time is None:\n",
    "                print('initial time as been setted to',initial_time)\n",
    "                initial_time = time.time()\n",
    "                \n",
    "            \n",
    "            elif initial_time is not None:        \n",
    "                print('now the initial time is not null beacuse current time is ',initial_time)\n",
    "                print(time.time() - initial_time)\n",
    "                # If the patience has run out and the person is still not detected then set the status to False\n",
    "                # Also save the video by releasing the video writer and send a text message.\n",
    "                if  time.time() - initial_time >= patience:\n",
    "                    status = False\n",
    "                    print(\"inside the loop\")\n",
    "                    exit_time = datetime.datetime.now().strftime(\"%A, %I:%M:%S %p %d %B %Y\")\n",
    "                    # out.release()\n",
    "                    # print(out)\n",
    "                    initial_time = None\n",
    "                    de = deque([False] * len(de))\n",
    "                    body = \"Alert: \\nA Person Entered the Room at {} \\nLeft the room at {}\".format(entry_time, exit_time) \n",
    "                    if len(recognisedName) > 0: \n",
    "                        body = \"Alert: {} Has entered in your Room at {} \\n Left the room at {}\".format(recognisedName,entry_time,exit_time)\n",
    "                    print('message has been sent')\n",
    "                    send_message(body, info_dict)\n",
    "                    frame_index = 0\n",
    "                    cv2.destroyWindow(\"frame\")\n",
    "                    while frame_heap:\n",
    "                        # Pop the frame with the maximum percentage change (negated)\n",
    "                        neg_change_percentage, frame , date = heapq.heappop(frame_heap)\n",
    "                        change_percentage = -neg_change_percentage\n",
    "                        file_path = 'DetectedPerson'\n",
    "                        \n",
    "                        # Save the frame to disk\n",
    "                        cv2.imwrite(f'{file_path}/{date}.jpg', frame)\n",
    "                        frame_index += 1\n",
    "                    print(\"saved frame succesfully\")\n",
    "                    continue\n",
    "                    \n",
    "                \n",
    "    \n",
    "    # If a significant amount of detections (more than half of detection_thresh) has occurred then we reset the Initial Time.\n",
    "    elif status and sum(de) > (detection_thresh/2):\n",
    "        change_percentage = calculate_percentage_change(prevframe, frame)\n",
    "        change_initial_change = calculate_percentage_change(initial_frame,frame)\n",
    "        # print(change_percentage)\n",
    "        if change_percentage > 95 and sent == False: \n",
    "            sent = True\n",
    "            body = \"Alert: \\n Some one has changed the camera configuration at:{}\".format(datetime.datetime.now().strftime(\"%A, %I-%M-%S %p %d %B %Y\")) \n",
    "            send_message(body,info_dict)\n",
    "    \n",
    "        if change_initial_change > 60: \n",
    "            heapq.heappush(frame_heap, (-change_percentage, frame,datetime.datetime.now().strftime(\"%A, %I-%M-%S %p %d %B %Y\")))\n",
    "        prevframe = frame\n",
    "        initial_time = None\n",
    "    \n",
    "    # Get the current time in the required format\n",
    "    current_time = datetime.datetime.now().strftime(\"%A, %I:%M:%S %p %d %B %Y\")\n",
    "\n",
    "    # Display the FPS\n",
    "    cv2.putText(annotated_image, 'FPS: {:.2f}'.format(fps), (510, 450), cv2.FONT_HERSHEY_COMPLEX, 0.6, (255, 40, 155), 2)\n",
    "    \n",
    "    # Display Time\n",
    "    cv2.putText(annotated_image, current_time, (310, 20), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 255), 1)    \n",
    "    \n",
    "    # Display the Room Status\n",
    "    cv2.putText(annotated_image, 'Room Occupied: {}'.format(str(status)), (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, \n",
    "                (200, 10, 150), 2)\n",
    "\n",
    "    # Show the patience Value\n",
    "    if initial_time is None:\n",
    "        text = 'Patience: {}'.format(patience)\n",
    "    else: \n",
    "        text = 'Patience: {:.2f}'.format(max(0, patience - (time.time() - initial_time)))\n",
    "        \n",
    "    cv2.putText(annotated_image, text, (10, 450), cv2.FONT_HERSHEY_COMPLEX, 0.6, (255, 40, 155), 2)   \n",
    "\n",
    " \n",
    "    # Show the Frame\n",
    "    cv2.imshow('frame', frame)\n",
    "    \n",
    "    # Calculate the Average FPS\n",
    "    frame_counter += 1\n",
    "    fps = (frame_counter / (time.time() - start_time))\n",
    "    \n",
    "    \n",
    "    # Exit if q is pressed.\n",
    "    if cv2.waitKey(30) == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "# Release Capture and destroy windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d4dc20-9b83-48d4-a5e9-add1b3fc7ae8",
   "metadata": {},
   "source": [
    "## D. Final Code Of ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b997d7c9-e00e-4ed3-8f62-37b87886134f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries succesfully imported\n",
      "Hello\n",
      "{0: 'Pushkar', 1: 'Sairaj', 2: 'Tushar'}\n",
      "no error in functions\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from collections import Counter\n",
    "import keras\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import datetime\n",
    "from collections import deque\n",
    "from twilio.rest import Client \n",
    "import random\n",
    "import heapq\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import requests\n",
    "import keras\n",
    "from keras import layers, Sequential\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "import mediapipe as mp\n",
    "import glob\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Libraries succesfully imported\") \n",
    "\n",
    "mp1 = keras.models.load_model(\"Models/FaceRecognitionModel.keras\")\n",
    "\n",
    "\n",
    "path_saved_model = \"Models/Action Recognition/model.keras\"\n",
    "threshold = 0.2\n",
    "save = \"Output\"\n",
    "print('Hello')\n",
    "##############\n",
    "torso_size_multiplier = 2.5\n",
    "n_landmarks = 33\n",
    "n_dimensions = 3\n",
    "landmark_names = [\n",
    "    'nose',\n",
    "    'left_eye_inner', 'left_eye', 'left_eye_outer',\n",
    "    'right_eye_inner', 'right_eye', 'right_eye_outer',\n",
    "    'left_ear', 'right_ear',\n",
    "    'mouth_left', 'mouth_right',\n",
    "    'left_shoulder', 'right_shoulder',\n",
    "    'left_elbow', 'right_elbow',\n",
    "    'left_wrist', 'right_wrist',\n",
    "    'left_pinky_1', 'right_pinky_1',\n",
    "    'left_index_1', 'right_index_1',\n",
    "    'left_thumb_2', 'right_thumb_2',\n",
    "    'left_hip', 'right_hip',\n",
    "    'left_knee', 'right_knee',\n",
    "    'left_ankle', 'right_ankle',\n",
    "    'left_heel', 'right_heel',\n",
    "    'left_foot_index', 'right_foot_index',\n",
    "]\n",
    "class_names = [\n",
    "    'Running','Standing','Walking','Waving'\n",
    "]\n",
    "\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "col_names = []\n",
    "for i in range(n_landmarks):\n",
    "    name = mp_pose.PoseLandmark(i).name\n",
    "    name_x = name + '_X'\n",
    "    name_y = name + '_Y'\n",
    "    name_z = name + '_Z'\n",
    "    name_v = name + '_V'\n",
    "    col_names.append(name_x)\n",
    "    col_names.append(name_y)\n",
    "    col_names.append(name_z)\n",
    "    col_names.append(name_v)\n",
    "\n",
    "# Load saved model\n",
    "model = load_model(path_saved_model, compile=True)\n",
    "\n",
    "\n",
    "# Load the face classifier\n",
    "face_classifier = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "# Load the face map for label mapping\n",
    "with open(\"ResultsMap.pkl\", 'rb') as fileReadStream:\n",
    "    ResultMap = pickle.load(fileReadStream)\n",
    "\n",
    "print(ResultMap)\n",
    "\n",
    "recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "recognizer.read(\"Models/ModelToRecogniseUnrecogniseFace.yml\")\n",
    "\n",
    "\n",
    "# Function to recognize faces\n",
    "def recognize_faces(frame):\n",
    "    # Convert the frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in the frame\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "    predicted_label = ''\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Extract the face ROI\n",
    "        face_roi = frame[y:y+h, x:x+w]\n",
    "        face_roi1 = gray[y:y+h, x:x+w]\n",
    "\n",
    "        # Resize the face ROI to match the input size of the model\n",
    "        label_id, confidence = recognizer.predict(face_roi1)\n",
    "\n",
    "        # Resize the face ROI to match the input size of the model\n",
    "        resized_face = cv2.resize(face_roi, (64, 64))\n",
    "\n",
    "        # Preprocess the face (normalize pixel values)\n",
    "        normalized_face = resized_face / 255.0\n",
    "\n",
    "        # Reshape the face for the model input (add batch dimension)\n",
    "        reshaped_face = np.expand_dims(normalized_face, axis=0)\n",
    "\n",
    "        # Perform prediction using the model\n",
    "        prediction = mp1.predict(reshaped_face)\n",
    "        predicted_label = ResultMap[np.argmax(prediction)]\n",
    "        print('confidence',prediction[0][np.argmax(prediction)])\n",
    "        print(prediction)\n",
    "        # print(predicted_label)\n",
    "        print(np.max(prediction))\n",
    "        name = predicted_label\n",
    "        pre = np.max(prediction)\n",
    "        print(confidence)\n",
    "        if confidence > 70: \n",
    "            cv2.putText(frame, 'Unknown', (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2) \n",
    "            name = 'Unknown'\n",
    "        else:\n",
    "            cv2.putText(frame, predicted_label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n",
    "\n",
    "        print(name)\n",
    "        # Draw the bounding box and label on the frame\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "    return predicted_label , frame\n",
    "\n",
    "\n",
    "def recognisize_pose(frame): \n",
    "    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = pose.process(img_rgb)\n",
    "    activity = ''\n",
    "    if result.pose_landmarks:\n",
    "        lm_list = []\n",
    "        for landmarks in result.pose_landmarks.landmark:\n",
    "            # Preprocessing\n",
    "            max_distance = 0\n",
    "            lm_list.append(landmarks)\n",
    "        # print('landmarks')\n",
    "        # print(landmarks)\n",
    "        center_x = (lm_list[landmark_names.index('right_hip')].x +\n",
    "                    lm_list[landmark_names.index('left_hip')].x)*0.5\n",
    "        center_y = (lm_list[landmark_names.index('right_hip')].y +\n",
    "                    lm_list[landmark_names.index('left_hip')].y)*0.5\n",
    "\n",
    "        shoulders_x = (lm_list[landmark_names.index('right_shoulder')].x +\n",
    "                       lm_list[landmark_names.index('left_shoulder')].x)*0.5\n",
    "        shoulders_y = (lm_list[landmark_names.index('right_shoulder')].y +\n",
    "                       lm_list[landmark_names.index('left_shoulder')].y)*0.5\n",
    "\n",
    "        for lm in lm_list:\n",
    "            distance = math.sqrt((lm.x - center_x)**2 + (lm.y - center_y)**2)\n",
    "            if(distance > max_distance):\n",
    "                max_distance = distance\n",
    "        torso_size = math.sqrt((shoulders_x - center_x) **\n",
    "                               2 + (shoulders_y - center_y)**2)\n",
    "        max_distance = max(torso_size*torso_size_multiplier, max_distance)\n",
    "\n",
    "        pre_lm = list(np.array([[(landmark.x-center_x)/max_distance, (landmark.y-center_y)/max_distance,\n",
    "                                 landmark.z/max_distance, landmark.visibility] for landmark in lm_list]).flatten())\n",
    "        data = pd.DataFrame([pre_lm], columns=col_names)\n",
    "        # print(data)\n",
    "        predict = model.predict(data)[0]\n",
    "        print('predicted')\n",
    "        activity = class_names[predict.argmax()]\n",
    "        if max(predict) > threshold:\n",
    "            pose_class = class_names[predict.argmax()]\n",
    "            print('predictions: ', predict)\n",
    "            print('predicted Pose Class: ', pose_class)\n",
    "        else:\n",
    "            pose_class = 'Unknown Pose'\n",
    "            print('[INFO] Predictions is below given Confidence!!')\n",
    "\n",
    "        frame = cv2.putText(\n",
    "                frame, f'{class_names[predict.argmax()]}',\n",
    "                (40, 50), cv2.FONT_HERSHEY_PLAIN,\n",
    "                2, (255, 0, 255), 2\n",
    "        )\n",
    "    \n",
    "    print('[INFO] Inference on Test Image is Ended...')\n",
    "    return activity \n",
    "\n",
    "\n",
    "\n",
    "def is_person_present(frame, thresh=1100):\n",
    "    kernel = None\n",
    "    global foog\n",
    "    \n",
    "    # Apply background subtraction\n",
    "    fgmask = foog.apply(frame)\n",
    "\n",
    "    # Get rid of the shadows\n",
    "    ret, fgmask = cv2.threshold(fgmask, 250, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Apply some morphological operations to make sure you have a good mask\n",
    "    fgmask = cv2.dilate(fgmask, kernel, iterations=4)\n",
    "\n",
    "    # Detect contours in the frame\n",
    "    contours, hierarchy = cv2.findContours(fgmask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "     \n",
    "    # Check if there was a contour and the area is somewhat higher than some threshold so we know it's a person and not noise\n",
    "    if contours and cv2.contourArea(max(contours, key=cv2.contourArea)) > thresh:\n",
    "            \n",
    "        # Get the max contour\n",
    "        cnt = max(contours, key=cv2.contourArea)\n",
    "\n",
    "        # Draw a bounding box around the person and label it as person detected\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
    "        cv2.putText(frame, 'Person Detected', (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "        \n",
    "        return True, frame\n",
    "        \n",
    "        \n",
    "    # Otherwise report there was no one present\n",
    "    else:\n",
    "        return False, frame\n",
    "\n",
    "\n",
    "def send_message(body, info_dict):\n",
    "\n",
    "    # Your Account SID from twilio.com/console\n",
    "    account_sid = info_dict['account_sid']\n",
    "\n",
    "    # Your Auth Token from twilio.com/console\n",
    "    auth_token  = info_dict['auth_token']\n",
    "\n",
    "\n",
    "    client = Client(account_sid, auth_token)\n",
    "\n",
    "    message = client.messages.create(to=info_dict['your_num'], from_=info_dict['trial_num'], body=body)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_percentage_change(frame1, frame2):\n",
    "    # Convert frames to grayscale\n",
    "    gray_frame1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "    gray_frame2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Calculate absolute difference between the two frames\n",
    "    abs_diff = cv2.absdiff(gray_frame1, gray_frame2)\n",
    "\n",
    "    # Calculate percentage change\n",
    "    percentage_change = (np.count_nonzero(abs_diff) / abs_diff.size) * 100\n",
    "\n",
    "    return percentage_change\n",
    "\n",
    "\n",
    "def save_frame(frame_heap): \n",
    "    frame_index = 0\n",
    "    while frame_heap:\n",
    "        # Pop the frame with the maximum percentage change (negated)\n",
    "        neg_change_percentage, frame , date = heapq.heappop(frame_heap)\n",
    "        change_percentage = -neg_change_percentage\n",
    "        file_path = 'DetectedPerson'\n",
    "        # Save the frame to disk\n",
    "        cv2.imwrite(f'{file_path}/{date}.jpg', frame)\n",
    "        frame_index += 1\n",
    "    print(\"saved frame succesfully\")\n",
    "\n",
    "print('no error in functions')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54c0e123-96a4-460a-ad5b-23ef83416578",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "predicted\n",
      "predictions:  [0.08939882 0.86259836 0.02882284 0.01917993]\n",
      "predicted Pose Class:  Standing\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "predicted\n",
      "predictions:  [0.22426225 0.6911128  0.04584526 0.03877966]\n",
      "predicted Pose Class:  Standing\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "predicted\n",
      "predictions:  [0.09881219 0.85404944 0.03410117 0.01303716]\n",
      "predicted Pose Class:  Standing\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "predicted\n",
      "predictions:  [0.02628325 0.94940805 0.01638214 0.00792656]\n",
      "predicted Pose Class:  Standing\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "predicted\n",
      "predictions:  [0.03164544 0.941232   0.01745842 0.00966411]\n",
      "predicted Pose Class:  Standing\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "predicted\n",
      "predictions:  [0.00484339 0.98568904 0.00749849 0.00196917]\n",
      "predicted Pose Class:  Standing\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "predicted\n",
      "predictions:  [0.01141437 0.97165686 0.0122493  0.00467938]\n",
      "predicted Pose Class:  Standing\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "confidence 0.3380075\n",
      "[[0.3325498  0.3380075  0.32944268]]\n",
      "0.3380075\n",
      "161.76180032570923\n",
      "Unknown\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "Sairaj\n",
      "i reached deepest of the darkest point\n",
      "initial time as been setted to None\n",
      "Sairaj\n",
      "i reached deepest of the darkest point\n",
      "now the initial time is not null beacuse current time is  1713216953.4614325\n",
      "0.11229562759399414\n",
      "inside the loop\n",
      "message has been sent\n",
      "saved frame succesfully\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "predicted\n",
      "predictions:  [0.02237724 0.92775524 0.03158735 0.01828015]\n",
      "predicted Pose Class:  Standing\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "predicted\n",
      "predictions:  [0.07785777 0.73962784 0.04700666 0.13550775]\n",
      "predicted Pose Class:  Standing\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "predicted\n",
      "predictions:  [0.08164532 0.5873477  0.03599219 0.29501483]\n",
      "predicted Pose Class:  Standing\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "\n",
      "inside block\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "predicted\n",
      "predictions:  [2.2714025e-01 2.6155743e-01 5.1084602e-01 4.5626613e-04]\n",
      "predicted Pose Class:  Walking\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "predicted\n",
      "predictions:  [0.40655732 0.2847821  0.30820692 0.00045365]\n",
      "predicted Pose Class:  Running\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "\n",
      "i reached deepest of the darkest point\n",
      "initial time as been setted to None\n",
      "\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "predicted\n",
      "predictions:  [0.3870945  0.2631666  0.34932414 0.00041476]\n",
      "predicted Pose Class:  Running\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "predicted\n",
      "predictions:  [5.6084764e-01 2.5145057e-01 1.8735783e-01 3.4402241e-04]\n",
      "predicted Pose Class:  Running\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "\n",
      "i reached deepest of the darkest point\n",
      "initial time as been setted to None\n",
      "\n",
      "i reached deepest of the darkest point\n",
      "now the initial time is not null beacuse current time is  1713216964.0562708\n",
      "0.10176539421081543\n",
      "inside the loop\n",
      "message has been sent\n",
      "saved frame succesfully\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "predicted\n",
      "predictions:  [3.6220962e-01 1.0689893e-01 5.3049725e-01 3.9412099e-04]\n",
      "predicted Pose Class:  Walking\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "predicted\n",
      "predictions:  [3.0879763e-01 1.3258497e-01 5.5825269e-01 3.6469122e-04]\n",
      "predicted Pose Class:  Walking\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "predicted\n",
      "predictions:  [2.9396427e-01 1.0103705e-01 6.0466099e-01 3.3776130e-04]\n",
      "predicted Pose Class:  Walking\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "predicted\n",
      "predictions:  [3.4266007e-01 1.0249388e-01 5.5456561e-01 2.8040202e-04]\n",
      "predicted Pose Class:  Walking\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "predicted\n",
      "predictions:  [3.2553491e-01 1.4883935e-01 5.2529526e-01 3.3043922e-04]\n",
      "predicted Pose Class:  Walking\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "predicted\n",
      "predictions:  [2.9948953e-01 1.6082910e-01 5.3938699e-01 2.9440047e-04]\n",
      "predicted Pose Class:  Walking\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "predicted\n",
      "predictions:  [3.8695478e-01 1.1275561e-01 4.9989083e-01 3.9880772e-04]\n",
      "predicted Pose Class:  Walking\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "\n",
      "i reached deepest of the darkest point\n",
      "initial time as been setted to None\n",
      "\n",
      "i reached deepest of the darkest point\n",
      "now the initial time is not null beacuse current time is  1713216978.9349673\n",
      "0.15827512741088867\n",
      "inside the loop\n",
      "message has been sent\n",
      "saved frame succesfully\n",
      "\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "predicted\n",
      "predictions:  [3.5056576e-01 6.7933299e-02 5.8113497e-01 3.6592194e-04]\n",
      "predicted Pose Class:  Walking\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "predicted\n",
      "predictions:  [3.1368354e-01 5.9430946e-02 6.2650448e-01 3.8104691e-04]\n",
      "predicted Pose Class:  Walking\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "\n",
      "\n",
      "\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "predicted\n",
      "predictions:  [4.2695290e-01 5.3318977e-02 5.1934665e-01 3.8150948e-04]\n",
      "predicted Pose Class:  Walking\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "predicted\n",
      "predictions:  [4.1818708e-01 6.1575200e-02 5.1987273e-01 3.6502499e-04]\n",
      "predicted Pose Class:  Walking\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "\n",
      "\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "predicted\n",
      "predictions:  [4.8814264e-01 9.5058091e-02 4.1641334e-01 3.8593923e-04]\n",
      "predicted Pose Class:  Running\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "predicted\n",
      "predictions:  [4.7022483e-01 8.8501632e-02 4.4088033e-01 3.9322409e-04]\n",
      "predicted Pose Class:  Running\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "predicted\n",
      "predictions:  [2.7507094e-01 1.0802787e-01 6.1656296e-01 3.3819335e-04]\n",
      "predicted Pose Class:  Walking\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "predicted\n",
      "predictions:  [3.1149849e-01 1.0720128e-01 5.8095419e-01 3.4602548e-04]\n",
      "predicted Pose Class:  Walking\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "predicted\n",
      "predictions:  [3.2659614e-01 1.0429131e-01 5.6868088e-01 4.3158987e-04]\n",
      "predicted Pose Class:  Walking\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "predicted\n",
      "predictions:  [4.5623183e-01 8.4550776e-02 4.5877755e-01 4.3987000e-04]\n",
      "predicted Pose Class:  Walking\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "predicted\n",
      "predictions:  [4.1673312e-01 8.0105461e-02 5.0275248e-01 4.0886737e-04]\n",
      "predicted Pose Class:  Walking\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "predicted\n",
      "predictions:  [3.3452162e-01 7.0573218e-02 5.9454018e-01 3.6497656e-04]\n",
      "predicted Pose Class:  Walking\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "predicted\n",
      "predictions:  [2.8884515e-01 5.5508092e-02 6.5529412e-01 3.5263103e-04]\n",
      "predicted Pose Class:  Walking\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "predicted\n",
      "predictions:  [5.0032699e-01 1.3870122e-01 3.6063802e-01 3.3381212e-04]\n",
      "predicted Pose Class:  Running\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "saved frame succesfully\n",
      "saved frame succesfully\n",
      "saved frame succesfully\n",
      "saved frame succesfully\n",
      "saved frame succesfully\n",
      "saved frame succesfully\n",
      "saved frame succesfully\n",
      "saved frame succesfully\n",
      "saved frame succesfully\n",
      "saved frame succesfully\n"
     ]
    }
   ],
   "source": [
    "# Set Window normal so we can resize it\n",
    "cv2.namedWindow('frame', cv2.WINDOW_NORMAL)\n",
    "\n",
    "# This is a test video\n",
    "# cap = cv2.VideoCapture(\"sample_video1.mp4\")\n",
    "\n",
    "# Read the video stream from the camera\n",
    "cap = cv2.VideoCapture('http://192.168.43.1:8080/video')\n",
    "\n",
    "# Get width and height of the frame\n",
    "width = int(cap.get(3))\n",
    "height = int(cap.get(4))\n",
    "\n",
    "# Read and store the credentials information in a dict\n",
    "with open('credential.txt', 'r') as myfile:\n",
    "  data = myfile.read()\n",
    "\n",
    "info_dict = eval(data)\n",
    "\n",
    "# Initialize the background Subtractor\n",
    "foog = cv2.createBackgroundSubtractorMOG2(detectShadows=True, varThreshold=100, history=2000)\n",
    "\n",
    "# Status is True when person is present and False when the person is not present.\n",
    "status = False\n",
    "\n",
    "# After the person disappears from view, wait at least 7 seconds before making the status False\n",
    "patience = 0.099\n",
    "\n",
    "# We don't consider an initial detection unless it's detected 15 times, this gets rid of false positives\n",
    "detection_thresh = 5\n",
    "\n",
    "# Initial time for calculating if patience time is up\n",
    "initial_time = None\n",
    "\n",
    "\n",
    "\n",
    "# We are creating a deque object of length detection_thresh and will store individual detection statuses here\n",
    "de = deque([False] * 10, maxlen=10)\n",
    "\n",
    "# Initialize an empty heap to store frames along with their negated percentage change\n",
    "frame_heap = []\n",
    "\n",
    "\n",
    "# Initialize these variables for calculating FPS\n",
    "fps = 0 \n",
    "frame_counter = 0\n",
    "start_time = time.time()\n",
    "counter = 0\n",
    "\n",
    "ret,prevframe = cap.read()\n",
    "initial_frame = prevframe\n",
    "recognisedName = \"\"\n",
    "sent  = False\n",
    "activity = []\n",
    "\n",
    "while True:\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break \n",
    "    \n",
    "    print(recognisedName)\n",
    "    # This function will return a boolean variable telling if someone was present or not, it will also draw boxes if it \n",
    "    # finds someone\n",
    "    detected, annotated_image = is_person_present(frame)  \n",
    "    # print(recognisedName)\n",
    "    # print('Hello')\n",
    "    \n",
    "\n",
    "    if detected and len(recognisedName) == 0: \n",
    "        # result = recogniseFromFace(frame)  \n",
    "        print('inside block')\n",
    "        result = recognize_faces(frame)  \n",
    "        # print(result)\n",
    "        activties = recognisize_pose(frame)\n",
    "        # print(activties)\n",
    "        activity.append(activties) \n",
    "        name , frame = result\n",
    "        print('inside block')\n",
    "        if name is not None: \n",
    "            if name: \n",
    "                recognisedName = name \n",
    "            else: \n",
    "                recognisedName = \"\"\n",
    "    # Register the current detection status on our deque object\n",
    "    de.appendleft(detected)\n",
    "    \n",
    "    # If we have consecutively detected a person 15 times then we are sure that someone is present    \n",
    "    # We also make this is the first time that this person has been detected so we only initialize the videowriter once\n",
    "    if sum(de) == detection_thresh and not status:                       \n",
    "            status = True\n",
    "            entry_time = datetime.datetime.now().strftime(\"%A, %I-%M-%S %p %d %B %Y\")\n",
    "            # out = cv2.VideoWriter('outputs/{}.mp4'.format(entry_time), cv2.VideoWriter_fourcc(*'XVID'), 15.0, (width, height))\n",
    "\n",
    "    # If status is True but the person is not in the current frame\n",
    "    if status and not detected:\n",
    "        \n",
    "        # Restart the patience timer only if the person has not been detected for a few frames so we are sure it wasn't a \n",
    "        # False positive\n",
    "        print('i reached deepest of the darkest point')\n",
    "        if sum(de) > (detection_thresh/2): \n",
    "            if initial_time is None:\n",
    "                print('initial time as been setted to',initial_time)\n",
    "                initial_time = time.time()\n",
    "                \n",
    "            \n",
    "            elif initial_time is not None:        \n",
    "                print('now the initial time is not null beacuse current time is ',initial_time)\n",
    "                print(time.time() - initial_time)\n",
    "                # If the patience has run out and the person is still not detected then set the status to False\n",
    "                # Also save the video by releasing the video writer and send a text message.\n",
    "                if  time.time() - initial_time >= patience:\n",
    "                    status = False\n",
    "                    print(\"inside the loop\")\n",
    "                    exit_time = datetime.datetime.now().strftime(\"%A, %I:%M:%S %p %d %B %Y\")\n",
    "                    # out.release()\n",
    "                    # print(out)\n",
    "                    initial_time = None\n",
    "                    de = deque([False] * len(de))\n",
    "                    element_counts = Counter(activity)\n",
    "                    \n",
    "                    # Find the most common element\n",
    "                    most_common_element = element_counts.most_common(1)\n",
    "                    if most_common_element:\n",
    "                        most_common_element = most_common_element[0][0]\n",
    "                        # Do something with most_common_element\n",
    "                    else:\n",
    "                        # Handle the case when there are no elements\n",
    "                        most_common_element = 'unknown activites'\n",
    "                    body = \"Alert: \\nA Person Entered the Room at {} \\nLeft the room at {} and did \".format(entry_time, exit_time,most_common_element) \n",
    "                    # Use Counter to count occurrences of each element\n",
    "                    activity.clear()\n",
    "                    if len(recognisedName) > 0: \n",
    "                        body = \"Alert: {} Has entered in your Room at {} \\n Left the room at {} and did {}\".format(recognisedName,entry_time,exit_time, most_common_element)\n",
    "                    print('message has been sent')\n",
    "                    send_message(body, info_dict)\n",
    "                    frame_index = 0\n",
    "                    cv2.destroyWindow(\"frame\")\n",
    "                    while frame_heap:\n",
    "                        # Pop the frame with the maximum percentage change (negated)\n",
    "                        neg_change_percentage, frame , date = heapq.heappop(frame_heap)\n",
    "                        change_percentage = -neg_change_percentage\n",
    "                        file_path = 'DetectedPerson'\n",
    "                        \n",
    "                        # Save the frame to disk\n",
    "                        cv2.imwrite(f'{file_path}/{date}.jpg', frame)\n",
    "                        frame_index += 1\n",
    "                    print(\"saved frame succesfully\")\n",
    "                    recognisedName = \"\" \n",
    "                    activity = []\n",
    "                    continue\n",
    "                    \n",
    "                \n",
    "    \n",
    "    # If a significant amount of detections (more than half of detection_thresh) has occurred then we reset the Initial Time.\n",
    "    elif status and sum(de) > (detection_thresh/2):\n",
    "        change_percentage = calculate_percentage_change(prevframe, frame)\n",
    "        change_initial_change = calculate_percentage_change(initial_frame,frame)\n",
    "        # print(change_percentage)\n",
    "        if change_percentage > 95 and sent == False: \n",
    "            sent = True\n",
    "            body = \"Alert: \\n Some one has changed the camera configuration at:{}\".format(datetime.datetime.now().strftime(\"%A, %I-%M-%S %p %d %B %Y\")) \n",
    "            if recognisedName != 'Unknown': \n",
    "                body = \"Alert: \\n {} has changed the camera configuration at:{}\".format(recognisedName, datetime.datetime.now().strftime(\"%A, %I-%M-%S %p %d %B %Y\")) \n",
    "            send_message(body,info_dict)\n",
    "    \n",
    "        if change_initial_change > 60: \n",
    "            heapq.heappush(frame_heap, (-change_percentage, frame,datetime.datetime.now().strftime(\"%A, %I-%M-%S %p %d %B %Y\")))\n",
    "        prevframe = frame\n",
    "        initial_time = None\n",
    "    \n",
    "    # Get the current time in the required format\n",
    "    current_time = datetime.datetime.now().strftime(\"%A, %I:%M:%S %p %d %B %Y\")\n",
    "\n",
    "    # Display the FPS\n",
    "    cv2.putText(annotated_image, 'FPS: {:.2f}'.format(fps), (510, 450), cv2.FONT_HERSHEY_COMPLEX, 0.6, (255, 40, 155), 2)\n",
    "    \n",
    "    # Display Time\n",
    "    cv2.putText(annotated_image, current_time, (310, 20), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 255), 1)    \n",
    "    \n",
    "    # Display the Room Status\n",
    "    cv2.putText(annotated_image, 'Room Occupied: {}'.format(str(status)), (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, \n",
    "                (200, 10, 150), 2)\n",
    "\n",
    "    # Show the patience Value\n",
    "    if initial_time is None:\n",
    "        text = 'Patience: {}'.format(patience)\n",
    "    else: \n",
    "        text = 'Patience: {:.2f}'.format(max(0, patience - (time.time() - initial_time)))\n",
    "        \n",
    "    cv2.putText(annotated_image, text, (10, 450), cv2.FONT_HERSHEY_COMPLEX, 0.6, (255, 40, 155), 2)   \n",
    "\n",
    " \n",
    "    # Show the Frame\n",
    "    cv2.imshow('frame', frame)\n",
    "    \n",
    "    # Calculate the Average FPS\n",
    "    frame_counter += 1\n",
    "    fps = (frame_counter / (time.time() - start_time))\n",
    "    \n",
    "    \n",
    "    # Exit if q is pressed.\n",
    "    if cv2.waitKey(30) == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "# Release Capture and destroy windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "while frame_heap:\n",
    "    # Pop the frame with the maximum percentage change (negated)\n",
    "    neg_change_percentage, frame , date = heapq.heappop(frame_heap)\n",
    "    change_percentage = -neg_change_percentage\n",
    "    file_path = 'DetectedPerson'\n",
    "                        \n",
    "    # Save the frame to disk\n",
    "    cv2.imwrite(f'{file_path}/{date}.jpg', frame)\n",
    "    frame_index += 1\n",
    "    print(\"saved frame succesfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f34bbbcc-9641-4f67-be10-5a2ab5323196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_9\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_9\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,432</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">51,264</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10816</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">692,288</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_18 (\u001b[38;5;33mConv2D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │           \u001b[38;5;34m2,432\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_18 (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_19 (\u001b[38;5;33mConv2D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m51,264\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_19 (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten_9 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10816\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_18 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │         \u001b[38;5;34m692,288\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_19 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                   │             \u001b[38;5;34m195\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,238,539</span> (8.54 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,238,539\u001b[0m (8.54 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">746,179</span> (2.85 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m746,179\u001b[0m (2.85 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,492,360</span> (5.69 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m1,492,360\u001b[0m (5.69 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806d4e6b-6092-4552-a1ec-40399a355fd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d9b04a-cc7f-46e1-80ae-d40c0a936462",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e8da809-16ec-438f-a21e-c3ef8ca6eeda",
   "metadata": {},
   "source": [
    "## Make it look Better and modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42c88a29-c8de-45e9-bca6-014be4243b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries succesfully imported\n",
      "Hello\n",
      "{0: 'Pushkar', 1: 'Sairaj', 2: 'Tushar'}\n",
      "no error in functions\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from collections import Counter\n",
    "import keras\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import datetime\n",
    "from collections import deque\n",
    "from twilio.rest import Client \n",
    "import random\n",
    "import heapq\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import requests\n",
    "import keras\n",
    "from keras import layers, Sequential\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "import mediapipe as mp\n",
    "import glob\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Libraries succesfully imported\") \n",
    "\n",
    "mp1 = keras.models.load_model(\"Models/FaceRecognitionModel.keras\")\n",
    "\n",
    "\n",
    "path_saved_model = \"Models/Action Recognition/model1.keras\"\n",
    "threshold = 0.2\n",
    "save = \"Output\"\n",
    "print('Hello')\n",
    "##############\n",
    "torso_size_multiplier = 2.5\n",
    "n_landmarks = 33\n",
    "n_dimensions = 3\n",
    "landmark_names = [\n",
    "    'nose',\n",
    "    'left_eye_inner', 'left_eye', 'left_eye_outer',\n",
    "    'right_eye_inner', 'right_eye', 'right_eye_outer',\n",
    "    'left_ear', 'right_ear',\n",
    "    'mouth_left', 'mouth_right',\n",
    "    'left_shoulder', 'right_shoulder',\n",
    "    'left_elbow', 'right_elbow',\n",
    "    'left_wrist', 'right_wrist',\n",
    "    'left_pinky_1', 'right_pinky_1',\n",
    "    'left_index_1', 'right_index_1',\n",
    "    'left_thumb_2', 'right_thumb_2',\n",
    "    'left_hip', 'right_hip',\n",
    "    'left_knee', 'right_knee',\n",
    "    'left_ankle', 'right_ankle',\n",
    "    'left_heel', 'right_heel',\n",
    "    'left_foot_index', 'right_foot_index',\n",
    "]\n",
    "class_names = [\n",
    "    'Running','Standing','Steal','Walking','Waving'\n",
    "]\n",
    "\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "col_names = []\n",
    "for i in range(n_landmarks):\n",
    "    name = mp_pose.PoseLandmark(i).name\n",
    "    name_x = name + '_X'\n",
    "    name_y = name + '_Y'\n",
    "    name_z = name + '_Z'\n",
    "    name_v = name + '_V'\n",
    "    col_names.append(name_x)\n",
    "    col_names.append(name_y)\n",
    "    col_names.append(name_z)\n",
    "    col_names.append(name_v)\n",
    "\n",
    "# Load saved model\n",
    "model = load_model(path_saved_model, compile=True)\n",
    "\n",
    "\n",
    "# Load the face classifier\n",
    "face_classifier = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "# Load the face map for label mapping\n",
    "with open(\"ResultsMap.pkl\", 'rb') as fileReadStream:\n",
    "    ResultMap = pickle.load(fileReadStream)\n",
    "\n",
    "print(ResultMap)\n",
    "\n",
    "recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "recognizer.read(\"Models/ModelToRecogniseUnrecogniseFace.yml\")\n",
    "\n",
    "\n",
    "# Function to recognize faces\n",
    "def recognize_faces(frame):\n",
    "    # Convert the frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in the frame\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "    predicted_label = ''\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Extract the face ROI\n",
    "        face_roi = frame[y:y+h, x:x+w]\n",
    "        face_roi1 = gray[y:y+h, x:x+w]\n",
    "\n",
    "        # Resize the face ROI to match the input size of the model\n",
    "        label_id, confidence = recognizer.predict(face_roi1)\n",
    "\n",
    "        # Resize the face ROI to match the input size of the model\n",
    "        resized_face = cv2.resize(face_roi, (64, 64))\n",
    "\n",
    "        # Preprocess the face (normalize pixel values)\n",
    "        normalized_face = resized_face / 255.0\n",
    "\n",
    "        # Reshape the face for the model input (add batch dimension)\n",
    "        reshaped_face = np.expand_dims(normalized_face, axis=0)\n",
    "\n",
    "        # Perform prediction using the model\n",
    "        prediction = mp1.predict(reshaped_face)\n",
    "        predicted_label = ResultMap[np.argmax(prediction)]\n",
    "        print('confidence',prediction[0][np.argmax(prediction)])\n",
    "        print(prediction)\n",
    "        # print(predicted_label)\n",
    "        print(np.max(prediction))\n",
    "        name = predicted_label\n",
    "        pre = np.max(prediction)\n",
    "        print(confidence)\n",
    "        if confidence > 70: \n",
    "            cv2.putText(frame, 'Unknown', (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2) \n",
    "            name = 'Unknown'\n",
    "        else:\n",
    "            cv2.putText(frame, predicted_label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n",
    "\n",
    "        print(name)\n",
    "        # Draw the bounding box and label on the frame\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "    return predicted_label , frame\n",
    "\n",
    "\n",
    "def recognisize_pose(frame): \n",
    "    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = pose.process(img_rgb)\n",
    "    activity = ''\n",
    "    if result.pose_landmarks:\n",
    "        lm_list = []\n",
    "        for landmarks in result.pose_landmarks.landmark:\n",
    "            # Preprocessing\n",
    "            max_distance = 0\n",
    "            lm_list.append(landmarks)\n",
    "        # print('landmarks')\n",
    "        # print(landmarks)\n",
    "        center_x = (lm_list[landmark_names.index('right_hip')].x +\n",
    "                    lm_list[landmark_names.index('left_hip')].x)*0.5\n",
    "        center_y = (lm_list[landmark_names.index('right_hip')].y +\n",
    "                    lm_list[landmark_names.index('left_hip')].y)*0.5\n",
    "\n",
    "        shoulders_x = (lm_list[landmark_names.index('right_shoulder')].x +\n",
    "                       lm_list[landmark_names.index('left_shoulder')].x)*0.5\n",
    "        shoulders_y = (lm_list[landmark_names.index('right_shoulder')].y +\n",
    "                       lm_list[landmark_names.index('left_shoulder')].y)*0.5\n",
    "\n",
    "        for lm in lm_list:\n",
    "            distance = math.sqrt((lm.x - center_x)**2 + (lm.y - center_y)**2)\n",
    "            if(distance > max_distance):\n",
    "                max_distance = distance\n",
    "        torso_size = math.sqrt((shoulders_x - center_x) **\n",
    "                               2 + (shoulders_y - center_y)**2)\n",
    "        max_distance = max(torso_size*torso_size_multiplier, max_distance)\n",
    "\n",
    "        pre_lm = list(np.array([[(landmark.x-center_x)/max_distance, (landmark.y-center_y)/max_distance,\n",
    "                                 landmark.z/max_distance, landmark.visibility] for landmark in lm_list]).flatten())\n",
    "        data = pd.DataFrame([pre_lm], columns=col_names)\n",
    "        # print(data)\n",
    "        predict = model.predict(data)[0]\n",
    "        print('predicted')\n",
    "        activity = class_names[predict.argmax()]\n",
    "        if max(predict) > threshold:\n",
    "            pose_class = class_names[predict.argmax()]\n",
    "            print('predictions: ', predict)\n",
    "            print('predicted Pose Class: ', pose_class)\n",
    "        else:\n",
    "            pose_class = 'Unknown Pose'\n",
    "            print('[INFO] Predictions is below given Confidence!!')\n",
    "\n",
    "        frame = cv2.putText(\n",
    "                frame, f'{class_names[predict.argmax()]}',\n",
    "                (40, 50), cv2.FONT_HERSHEY_PLAIN,\n",
    "                2, (255, 0, 255), 2\n",
    "        )\n",
    "    \n",
    "    print('[INFO] Inference on Test Image is Ended...')\n",
    "    return activity \n",
    "\n",
    "\n",
    "\n",
    "def is_person_present(frame, thresh=1100):\n",
    "    kernel = None\n",
    "    global foog\n",
    "    \n",
    "    # Apply background subtraction\n",
    "    fgmask = foog.apply(frame)\n",
    "\n",
    "    # Get rid of the shadows\n",
    "    ret, fgmask = cv2.threshold(fgmask, 250, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Apply some morphological operations to make sure you have a good mask\n",
    "    fgmask = cv2.dilate(fgmask, kernel, iterations=4)\n",
    "\n",
    "    # Detect contours in the frame\n",
    "    contours, hierarchy = cv2.findContours(fgmask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "     \n",
    "    # Check if there was a contour and the area is somewhat higher than some threshold so we know it's a person and not noise\n",
    "    if contours and cv2.contourArea(max(contours, key=cv2.contourArea)) > thresh:\n",
    "            \n",
    "        # Get the max contour\n",
    "        cnt = max(contours, key=cv2.contourArea)\n",
    "\n",
    "        # Draw a bounding box around the person and label it as person detected\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
    "        cv2.putText(frame, 'Person Detected', (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "        \n",
    "        return True, frame\n",
    "        \n",
    "        \n",
    "    # Otherwise report there was no one present\n",
    "    else:\n",
    "        return False, frame\n",
    "\n",
    "\n",
    "def send_message(body, info_dict):\n",
    "\n",
    "    # Your Account SID from twilio.com/console\n",
    "    account_sid = info_dict['account_sid']\n",
    "\n",
    "    # Your Auth Token from twilio.com/console\n",
    "    auth_token  = info_dict['auth_token']\n",
    "\n",
    "\n",
    "    client = Client(account_sid, auth_token)\n",
    "\n",
    "    message = client.messages.create(to=info_dict['your_num'], from_=info_dict['trial_num'], body=body)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_percentage_change(frame1, frame2):\n",
    "    # Convert frames to grayscale\n",
    "    gray_frame1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "    gray_frame2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Calculate absolute difference between the two frames\n",
    "    abs_diff = cv2.absdiff(gray_frame1, gray_frame2)\n",
    "\n",
    "    # Calculate percentage change\n",
    "    percentage_change = (np.count_nonzero(abs_diff) / abs_diff.size) * 100\n",
    "\n",
    "    return percentage_change\n",
    "\n",
    "\n",
    "def save_frame(frame_heap): \n",
    "    frame_index = 0\n",
    "    while frame_heap:\n",
    "        # Pop the frame with the maximum percentage change (negated)\n",
    "        neg_change_percentage, frame , date = heapq.heappop(frame_heap)\n",
    "        change_percentage = -neg_change_percentage\n",
    "        file_path = 'DetectedPerson'\n",
    "        # Save the frame to disk\n",
    "        cv2.imwrite(f'{file_path}/{date}.jpg', frame)\n",
    "        frame_index += 1\n",
    "    print(\"saved frame succesfully\")\n",
    "\n",
    "print('no error in functions')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64f00db5-a5ab-48d9-8ebf-22befe58d4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "inside block\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "confidence 0.3403757\n",
      "[[0.32300928 0.33661503 0.3403757 ]]\n",
      "0.3403757\n",
      "53.45232896172901\n",
      "Tushar\n",
      "[INFO] Inference on Test Image is Ended...\n",
      "inside block\n",
      "Tushar\n",
      "Tushar\n",
      "Tushar\n",
      "Tushar\n",
      "Tushar\n",
      "Tushar\n",
      "Tushar\n",
      "Tushar\n",
      "Tushar\n",
      "Tushar\n",
      "Tushar\n",
      "Tushar\n",
      "Tushar\n",
      "Tushar\n",
      "Tushar\n",
      "Tushar\n",
      "Tushar\n",
      "Tushar\n",
      "Tushar\n",
      "Tushar\n",
      "Tushar\n",
      "Tushar\n",
      "Tushar\n",
      "Tushar\n",
      "Tushar\n",
      "Tushar\n",
      "Tushar\n",
      "Tushar\n",
      "Tushar\n",
      "Tushar\n",
      "Tushar\n",
      "Tushar\n",
      "Tushar\n",
      "Tushar\n",
      "Tushar\n",
      "Tushar\n",
      "Tushar\n",
      "Tushar\n",
      "Tushar\n",
      "Tushar\n",
      "Tushar\n",
      "Tushar\n",
      "Tushar\n",
      "Tushar\n"
     ]
    }
   ],
   "source": [
    "# Set Window normal so we can resize it\n",
    "cv2.namedWindow('frame', cv2.WINDOW_NORMAL)\n",
    "\n",
    "# This is a test video\n",
    "# cap = cv2.VideoCapture(\"sample_video1.mp4\")\n",
    "\n",
    "# Read the video stream from the camera\n",
    "cap = cv2.VideoCapture('Video_5.mp4')\n",
    "\n",
    "# Get width and height of the frame\n",
    "width = int(cap.get(3))\n",
    "height = int(cap.get(4))\n",
    "\n",
    "# Read and store the credentials information in a dict\n",
    "with open('credential.txt', 'r') as myfile:\n",
    "  data = myfile.read()\n",
    "\n",
    "info_dict = eval(data)\n",
    "\n",
    "# Initialize the background Subtractor\n",
    "foog = cv2.createBackgroundSubtractorMOG2(detectShadows=True, varThreshold=100, history=2000)\n",
    "\n",
    "# Status is True when person is present and False when the person is not present.\n",
    "status = False\n",
    "\n",
    "# After the person disappears from view, wait at least 7 seconds before making the status False\n",
    "patience = 0.099\n",
    "\n",
    "# We don't consider an initial detection unless it's detected 15 times, this gets rid of false positives\n",
    "detection_thresh = 5\n",
    "\n",
    "# Initial time for calculating if patience time is up\n",
    "initial_time = None\n",
    "\n",
    "\n",
    "\n",
    "# We are creating a deque object of length detection_thresh and will store individual detection statuses here\n",
    "de = deque([False] * 10, maxlen=10)\n",
    "\n",
    "# Initialize an empty heap to store frames along with their negated percentage change\n",
    "frame_heap = []\n",
    "\n",
    "\n",
    "# Initialize these variables for calculating FPS\n",
    "fps = 0 \n",
    "frame_counter = 0\n",
    "start_time = time.time()\n",
    "counter = 0\n",
    "\n",
    "ret,prevframe = cap.read()\n",
    "initial_frame = prevframe\n",
    "recognisedName = \"\"\n",
    "sent  = False\n",
    "activity = []\n",
    "\n",
    "while True:\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break \n",
    "    \n",
    "    print(recognisedName)\n",
    "    # This function will return a boolean variable telling if someone was present or not, it will also draw boxes if it \n",
    "    # finds someone\n",
    "    detected, annotated_image = is_person_present(frame)  \n",
    "    # print(recognisedName)\n",
    "    # print('Hello')\n",
    "    \n",
    "\n",
    "    if detected and len(recognisedName) == 0: \n",
    "        # result = recogniseFromFace(frame)  \n",
    "        print('inside block')\n",
    "        result = recognize_faces(frame)  \n",
    "        # print(result)\n",
    "        activties = recognisize_pose(frame)\n",
    "        # print(activties)\n",
    "        activity.append(activties) \n",
    "        name , frame = result\n",
    "        print('inside block')\n",
    "        if name is not None: \n",
    "            if name: \n",
    "                recognisedName = name \n",
    "            else: \n",
    "                recognisedName = \"\"\n",
    "    # Register the current detection status on our deque object\n",
    "    de.appendleft(detected)\n",
    "    \n",
    "    # If we have consecutively detected a person 15 times then we are sure that someone is present    \n",
    "    # We also make this is the first time that this person has been detected so we only initialize the videowriter once\n",
    "    if sum(de) == detection_thresh and not status:                       \n",
    "            status = True\n",
    "            entry_time = datetime.datetime.now().strftime(\"%A, %I-%M-%S %p %d %B %Y\")\n",
    "            # out = cv2.VideoWriter('outputs/{}.mp4'.format(entry_time), cv2.VideoWriter_fourcc(*'XVID'), 15.0, (width, height))\n",
    "\n",
    "    # If status is True but the person is not in the current frame\n",
    "    if status and not detected:\n",
    "        \n",
    "        # Restart the patience timer only if the person has not been detected for a few frames so we are sure it wasn't a \n",
    "        # False positive\n",
    "        if sum(de) > (detection_thresh/2): \n",
    "            if initial_time is None:\n",
    "                print('initial time as been setted to',initial_time)\n",
    "                initial_time = time.time()\n",
    "                \n",
    "            \n",
    "            elif initial_time is not None:        \n",
    "                print(time.time() - initial_time)\n",
    "                # If the patience has run out and the person is still not detected then set the status to False\n",
    "                # Also save the video by releasing the video writer and send a text message.\n",
    "                if  time.time() - initial_time >= patience:\n",
    "                    if len(recognisedName) == 0 or recognisedName == 'Unknown':\n",
    "                        status = False\n",
    "                        print(\"inside the loop\")\n",
    "                        exit_time = datetime.datetime.now().strftime(\"%A, %I:%M:%S %p %d %B %Y\")\n",
    "                        # out.release()\n",
    "                        # print(out)\n",
    "                        initial_time = None\n",
    "                        de = deque([False] * len(de))\n",
    "                        element_counts = Counter(activity)\n",
    "                        \n",
    "                        # Find the most common element\n",
    "                        most_common_element = element_counts.most_common(1)\n",
    "                        if most_common_element:\n",
    "                            most_common_element = most_common_element[0][0]\n",
    "                            # Do something with most_common_element\n",
    "                        else:\n",
    "                            # Handle the case when there are no elements\n",
    "                            most_common_element = 'unknown activites'\n",
    "                        body = \"Alert: \\nA Person Entered the Room at {} \\nLeft the room at {} and did \".format(entry_time, exit_time,most_common_element) \n",
    "                        # Use Counter to count occurrences of each element\n",
    "                        activity.clear()\n",
    "                        print('message has been sent')\n",
    "                        send_message(body, info_dict)\n",
    "                        frame_index = 0\n",
    "                        cv2.destroyWindow(\"frame\")\n",
    "                        while frame_heap:\n",
    "                            # Pop the frame with the maximum percentage change (negated)\n",
    "                            neg_change_percentage, frame , date = heapq.heappop(frame_heap)\n",
    "                            change_percentage = -neg_change_percentage\n",
    "                            file_path = 'DetectedPerson'\n",
    "                            \n",
    "                            # Save the frame to disk\n",
    "                            cv2.imwrite(f'{file_path}/{date}.jpg', frame)\n",
    "                            frame_index += 1\n",
    "                        print(\"saved frame succesfully\")\n",
    "                        recognisedName = \"\" \n",
    "                        activity = []\n",
    "                        continue                \n",
    "    # If a significant amount of detections (more than half of detection_thresh) has occurred then we reset the Initial Time.\n",
    "    elif status and sum(de) > (detection_thresh/2):\n",
    "        change_percentage = calculate_percentage_change(prevframe, frame)\n",
    "        change_initial_change = calculate_percentage_change(initial_frame,frame)\n",
    "        # print(change_percentage)\n",
    "        if change_percentage > 95 and sent == False:\n",
    "            if len(recognisedName) == 0 or recognisedName == 'Unknown':\n",
    "                sent = True\n",
    "                body = \"Alert: \\n Some one has changed the camera configuration at:{}\".format(datetime.datetime.now().strftime(\"%A, %I-%M-%S %p %d %B %Y\")) \n",
    "                send_message(body,info_dict)\n",
    "    \n",
    "        if change_initial_change > 60: \n",
    "            heapq.heappush(frame_heap, (-change_percentage, frame,datetime.datetime.now().strftime(\"%A, %I-%M-%S %p %d %B %Y\")))\n",
    "        prevframe = frame\n",
    "        initial_time = None\n",
    "    \n",
    "    # Get the current time in the required format\n",
    "    current_time = datetime.datetime.now().strftime(\"%A, %I:%M:%S %p %d %B %Y\")\n",
    "\n",
    "    # Display the FPS\n",
    "    cv2.putText(annotated_image, 'FPS: {:.2f}'.format(fps), (510, 450), cv2.FONT_HERSHEY_COMPLEX, 0.6, (255, 40, 155), 2)\n",
    "    \n",
    "    # Display Time\n",
    "    cv2.putText(annotated_image, current_time, (310, 20), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 255), 1)    \n",
    "    \n",
    "    # Display the Room Status\n",
    "    cv2.putText(annotated_image, 'Room Occupied: {}'.format(str(status)), (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, \n",
    "                (200, 10, 150), 2)\n",
    "\n",
    "    # Show the patience Value\n",
    "    if initial_time is None:\n",
    "        text = 'Patience: {}'.format(patience)\n",
    "    else: \n",
    "        text = 'Patience: {:.2f}'.format(max(0, patience - (time.time() - initial_time)))\n",
    "        \n",
    "    cv2.putText(annotated_image, text, (10, 450), cv2.FONT_HERSHEY_COMPLEX, 0.6, (255, 40, 155), 2)   \n",
    "\n",
    " \n",
    "    # Show the Frame\n",
    "    cv2.imshow('frame', frame)\n",
    "    \n",
    "    # Calculate the Average FPS\n",
    "    frame_counter += 1\n",
    "    fps = (frame_counter / (time.time() - start_time))\n",
    "    \n",
    "    \n",
    "    # Exit if q is pressed.\n",
    "    if cv2.waitKey(30) == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "# Release Capture and destroy windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# while frame_heap:\n",
    "#     # Pop the frame with the maximum percentage change (negated)\n",
    "#     neg_change_percentage, frame , date = heapq.heappop(frame_heap)\n",
    "#     change_percentage = -neg_change_percentage\n",
    "#     file_path = 'DetectedPerson'\n",
    "                        \n",
    "#     # Save the frame to disk\n",
    "#     cv2.imwrite(f'{file_path}/{date}.jpg', frame)\n",
    "#     frame_index += 1\n",
    "#     print(\"saved frame succesfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc5a489-a960-4d49-bc12-ddf952a1eede",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
